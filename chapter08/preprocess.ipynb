{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data\n",
    "import os \n",
    "cu_path = os.getcwd()\n",
    "data_path = os.path.join(cu_path,\"data\",\"NewsAggregatorDataset\",\"newsCorpora.csv\")\n",
    "data_write_path = os.path.join(cu_path,\"data\",\"full_data.txt\")\n",
    "\n",
    "publisher_list = [\"Reuters\", \"Huffington Post\", \"Businessweek\", \"Contactmusic.com\", \"Daily Mail\"]\n",
    "\n",
    "\n",
    "with open(data_path) as read_file,open(data_write_path,\"w\") as write_file:\n",
    "    for line in read_file:\n",
    "        line = line.split(\"\\t\")\n",
    "        if line[3] in publisher_list:\n",
    "            write_file.write(line[1]+\"\\t\"+line[4]+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuflle\n",
    "#!gshuf data/full_data.txt > data/full_data_shuf.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   13356 data/full_data_shuf.txt\n"
     ]
    }
   ],
   "source": [
    "#divide\n",
    "!wc -l data/full_data_shuf.txt\n",
    "!head -n 10648 data/full_data_shuf.txt > data/train.txt\n",
    "!sed -n 10649,11984p data/full_data_shuf.txt > data/dev.txt\n",
    "!sed -n 11985,13356p data/full_data_shuf.txt > data/test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   10648 data/train.txt\n",
      "    1372 data/test.txt\n",
      "    1336 data/dev.txt\n"
     ]
    }
   ],
   "source": [
    "#check\n",
    "!wc -l data/train.txt\n",
    "!wc -l data/test.txt\n",
    "!wc -l data/dev.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sato_shun/.pyenv/versions/miniconda3-3.19.0/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "#70\n",
    "from gensim.models import KeyedVectors\n",
    "path='data/GoogleNews-vectors-negative300.bin'\n",
    "model=KeyedVectors.load_word2vec_format(path,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "#ここで名前処理して行数減らす\n",
    "name_list = [\"train\",\"dev\",\"test\"]\n",
    "label_list = [\"b\",\"t\",\"e\",\"m\"]\n",
    "cu_path = os.getcwd()\n",
    "for name in name_list:\n",
    "    read_path = os.path.join(cu_path,\"data\",name+\".txt\")\n",
    "    write_path_X = os.path.join(cu_path,\"data\",name+\"_X.csv\")\n",
    "    write_path_Y = os.path.join(cu_path,\"data\",name+\"_Y.csv\")\n",
    "    \n",
    "    length = sum(1 for line in open(read_path))\n",
    "    sent_vector = np.zeros((length,300))\n",
    "    label_vector = np.zeros((length,1))\n",
    "\n",
    "\n",
    "    with open(read_path) as read_file,open(write_path_X,\"w\") as write_file_X, open(write_path_Y,\"w\") as write_file_Y:\n",
    "      \n",
    "        for i,line in enumerate(read_file):\n",
    "            sent,category=line.strip().split(\"\\t\")\n",
    "#             print(sent)\n",
    "#             print(sent_vector)\n",
    "\n",
    "            #process for sentence\n",
    "            sent = sent.replace(\".\",\" .\").replace(\",\",\" ,\")\\\n",
    "                       .replace(\"!\",\" !\").replace(\";\",\" ;\").replace(\":\",\" :\")\\\n",
    "                       .replace('\"','').replace(\"'\",\"\").replace(\"?\",\" ?\").split()\n",
    "            #平均つけろ\n",
    "            count = 0\n",
    "            sent_vector_memory = 0\n",
    "            for word in sent:\n",
    "                if (word in model.vocab):\n",
    "                    count +=1\n",
    "                    sent_vector_memory +=  model[word]\n",
    "                    \n",
    "            sent_vector[i] = sent_vector_memory/count\n",
    "            \n",
    "            #process for label\n",
    "            label_vector[i]=int(label_list.index(category)) \n",
    "\n",
    "        writer_X=csv.writer(write_file_X)\n",
    "        writer_X.writerows(sent_vector)\n",
    "        \n",
    "        writer_Y=csv.writer(write_file_Y)\n",
    "        writer_Y.writerows(label_vector)\n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "#             print(sent_vector)\n",
    "\n",
    "    \n",
    "        \n",
    "                \n",
    "\n",
    "                \n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
