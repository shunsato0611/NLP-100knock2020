{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 80. ID番号への変換"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問題51で構築した学習データ中の単語にユニークなID番号を付与したい．学習データ中で最も頻出する単語に1，2番目に頻出する単語に2，……といった方法で，学習データ中で2回以上出現する単語にID番号を付与せよ．そして，与えられた単語列に対して，ID番号の列を返す関数を実装せよ．ただし，出現頻度が2回未満の単語のID番号はすべて0とせよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "cu_path = os.getcwd()\n",
    "data_path = os.path.join(cu_path,'data','full_data_tokened.txt')\n",
    "\n",
    "vocab = {}\n",
    "with open(data_path) as data:\n",
    "    for line in data:\n",
    "        text_data = line.split(\"\\t\")[0]\n",
    "        words = text_data.split()\n",
    "\n",
    "        for word in  words:\n",
    "            if word in vocab:\n",
    "                vocab[word] +=1\n",
    "            else:\n",
    "                vocab[word] = 1\n",
    "\n",
    "#あとでサンプルで使う\n",
    "sample = words\n",
    "\n",
    "#(単語,頻度)の順にタプルで入ってる\n",
    "vocab_sorted = sorted(vocab.items(),key=lambda x:x[1])[::-1]\n",
    "\n",
    "vocab_with_id = {}\n",
    "#idに変換\n",
    "for item in vocab_sorted:\n",
    "    if item[1] >= 2:\n",
    "        vocab_with_id[item[0]] = len(vocab_with_id.items())\n",
    "    else:\n",
    "        vocab_with_id[item[0]] = 0\n",
    "\n",
    "def word2id(words):\n",
    "    return [vocab_with_id[word] for word in words]\n",
    "#sample\n",
    "print(sample)\n",
    "print(word2id(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 81. RNNによる予測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID番号で表現された単語列x=(x1,x2,…,xT)\n",
    "がある．ただし，T\n",
    "は単語列の長さ，xt∈ℝV\n",
    "は単語のID番号のone-hot表記である（V\n",
    "は単語の総数である）．再帰型ニューラルネットワーク（RNN: Recurrent Neural Network）を用い，単語列x\n",
    "からカテゴリy\n",
    "を予測するモデルとして，次式を実装せよ．\n",
    "\n",
    "h→0=0,h→t=RNN−→−−(emb(xt),h→t−1),y=softmax(W(yh)h→T+b(y))\n",
    "ただし，emb(x)∈ℝdw\n",
    "は単語埋め込み（単語のone-hot表記から単語ベクトルに変換する関数），h→t∈ℝdh\n",
    "は時刻t\n",
    "の隠れ状態ベクトル，RNN−→−−(x,h)\n",
    "は入力x\n",
    "と前時刻の隠れ状態h\n",
    "から次状態を計算するRNNユニット，W(yh)∈ℝL×dh\n",
    "は隠れ状態ベクトルからカテゴリを予測するための行列，b(y)∈ℝL\n",
    "はバイアス項である（dw,dh,L\n",
    "はそれぞれ，単語埋め込みの次元数，隠れ状態ベクトルの次元数，ラベル数である）．RNNユニットRNN−→−−(x,h)\n",
    "には様々な構成が考えられるが，典型例として次式が挙げられる．\n",
    "\n",
    "RNN−→−−(x,h)=g(W(hx)x+W(hh)h+b(h))\n",
    "ただし，W(hx)∈ℝdh×dw，W(hh)∈ℝdh×dh,b(h)∈ℝdh\n",
    "はRNNユニットのパラメータ，g\n",
    "は活性化関数（例えばtanh\n",
    "やReLUなど）である．\n",
    "\n",
    "なお，この問題ではパラメータの学習を行わず，ランダムに初期化されたパラメータでy\n",
    "を計算するだけでよい．次元数などのハイパーパラメータは，dw=300,dh=50\n",
    "など，適当な値に設定せよ（以降の問題でも同様である）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "train_acc : 0.2926371149511645\n"
     ]
    }
   ],
   "source": [
    "#train dev test 前の章で作ったものを使う\n",
    "#full_data_tokened.txtは3つの全部が入ってる\n",
    "#mozes でトークナイズ済み\n",
    "import os \n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn \n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence,pad_sequence,pack_sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim,hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim,tagset_size)\n",
    "\n",
    "    def forward(self,sentence,word_seq_lengths):\n",
    "        embeds = self.word_embeddings(sentence)  \n",
    "\n",
    "        #手書きでpaddingしたのでpackしてRNNに入れて戻す\n",
    "        packed_words = pack_padded_sequence(embeds, word_seq_lengths.cpu().numpy(), batch_first=True)   \n",
    "        hidden = None\n",
    "        output, _ = self.rnn(packed_words,hidden)\n",
    "        # output, _ = self.rnn(embeds)\n",
    "        output, _ = pad_packed_sequence(output,batch_first=True)\n",
    "        \n",
    "        #ここ0の分処理してないので直す\n",
    "        output=torch.sum(output,dim=1)\n",
    "        \n",
    "        tag_space = self.hidden2tag(output)\n",
    " \n",
    "        tag_scores = F.log_softmax(tag_space,dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "#vocab 次元の辞書を作る 80番の内容\n",
    "def make_vocab(data_path):\n",
    "    vocab = {}\n",
    "    with open(data_path) as data:\n",
    "        for line in data:\n",
    "            text_data = line.split(\"\\t\")[0]\n",
    "            words = text_data.split()\n",
    "\n",
    "            for word in  words:\n",
    "                if word in vocab:\n",
    "                    vocab[word] +=1\n",
    "                else:\n",
    "                    vocab[word] = 1\n",
    "\n",
    "    #(単語,頻度)の順にタプルで入ってる\n",
    "    vocab_sorted = sorted(vocab.items(),key=lambda x:x[1])[::-1]\n",
    "\n",
    "    vocab_with_id = {}\n",
    "    #idに変換\n",
    "    for item in vocab_sorted:\n",
    "        if item[1] >= 2:\n",
    "            vocab_with_id[item[0]] = len(vocab_with_id.items())\n",
    "        else:\n",
    "            vocab_with_id[item[0]] = 0\n",
    "\n",
    "    return vocab_with_id\n",
    "\n",
    "#train,test,devを読み込む関数\n",
    "def data_import(mode,vocab_with_id):\n",
    "    cu_path = os.getcwd()\n",
    "    name = mode\n",
    "    data_path = os.path.join(cu_path,\"data\",name+\".txt\")\n",
    "\n",
    "    text_data_list = []\n",
    "    label_list = []\n",
    "    with open(data_path) as data:\n",
    "        for line in data:\n",
    "            text_data,label  = line.strip().split(\"\\t\")\n",
    "            words = text_data.split()\n",
    "            text_data_list.append(words)\n",
    "            label_list.append(label)\n",
    "    \n",
    "    return text_data_list,label_list\n",
    "\n",
    "#単語列 -> id に変換する\n",
    "def prepare_sequence(seqs,with_id,mode):\n",
    "    bacth_id_list = []\n",
    "    for seq in seqs:\n",
    "        idxs = [with_id[w] for w in seq]\n",
    "        bacth_id_list.append(idxs)\n",
    "    # print(bacth_id_list)\n",
    "    # return torch.tensor(idxs, dtype=torch.long)\n",
    "    return bacth_id_list\n",
    "\n",
    "#結局手で書いてしまった\n",
    "#系列長が違う生でlist > tensor の変換がうまくいかないので0埋めを手動でやった\n",
    "def sequence2padded_tesnsor(seqs,labels):    \n",
    "    batch_size = len(seqs)\n",
    "    word_seq_lengths = torch.LongTensor(list(map(len, seqs)))\n",
    "    max_seq_len = word_seq_lengths.max().item()\n",
    "    word_seq_tensor = torch.zeros((batch_size, max_seq_len), requires_grad=True).long()\n",
    "    label_seq_tensor = torch.zeros((batch_size), requires_grad=True).long()\n",
    "    for idx, (seq, label,seqlen) in enumerate(zip(seqs, labels,word_seq_lengths)):\n",
    "        seqlen = seqlen.item()\n",
    "        word_seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "        label_seq_tensor[idx] = torch.LongTensor(label)\n",
    "    \n",
    "    word_seq_lengths, word_perm_idx = word_seq_lengths.sort(0, descending=True)\n",
    "    word_seq_tensor = word_seq_tensor[word_perm_idx]#長さごとにに並べ替え\n",
    "    label_seq_tensor = label_seq_tensor[word_perm_idx]\n",
    "\n",
    "    return word_seq_tensor,label_seq_tensor,word_seq_lengths\n",
    "    \n",
    "def main():\n",
    "    #paraameter\n",
    "    EMBEDDING_DIM = 300\n",
    "    HIDDEN_DIM = 50\n",
    "    BATCH_SIZE = 32\n",
    "     \n",
    "    cu_path = os.getcwd()\n",
    "    data_path = os.path.join(cu_path,'data','full_data_tokened.txt')\n",
    "    vocab_with_id = make_vocab(data_path)\n",
    "    train_X, train_y = data_import(\"train\",vocab_with_id)\n",
    "\n",
    "    label_with_id = {\"b\":0,\"t\":1,\"e\":2,\"m\":3}\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model = SimpleRNN(EMBEDDING_DIM, \n",
    "                      HIDDEN_DIM, \n",
    "                      len(vocab_with_id), \n",
    "                      len(label_with_id))\n",
    "    loss_function = nn.NLLLoss()\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    for epoch in range(1):\n",
    "        print(epoch)\n",
    "        train_num = len(train_X)\n",
    "        batch_size = BATCH_SIZE\n",
    "\n",
    "        if train_num % batch_size == 0:\n",
    "            total_batch = train_num // batch_size \n",
    "        else:\n",
    "            total_batch = train_num // batch_size + 1\n",
    "\n",
    "        loss_total = 0\n",
    "        gold_list = np.array([-1])\n",
    "        pred_list = np.array([-1])\n",
    "        \n",
    "        for batch_id in range(total_batch):\n",
    "            #それぞれのsizeが違ってdata_loaderの使いかたがわからなかった\n",
    "            #全部一気に載せれなかったのでバッチで少しづつ載せる\n",
    "            start = batch_id * batch_size\n",
    "            end = (batch_id + 1) * batch_size\n",
    "            sentences = train_X[start:end]\n",
    "            tags = train_y[start:end]\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            sentence_in = prepare_sequence(sentences, vocab_with_id,\"X\")\n",
    "            targets = prepare_sequence(tags, label_with_id,\"y\")\n",
    "            sentence_in, targets, word_seq_lengths = sequence2padded_tesnsor(sentence_in,targets)\n",
    "\n",
    "            tag_scores = model(sentence_in,word_seq_lengths)\n",
    "            \n",
    "            # print(tag_scores)\n",
    "            \n",
    "            pred_y = torch.argmax(tag_scores,dim=1).numpy()\n",
    "            pred_list = np.insert(pred_list,-1,pred_y)\n",
    "        \n",
    "            gold_y = np.array(targets)\n",
    "            gold_list = np.insert(gold_list,-1,gold_y)\n",
    "            \n",
    "        pred_list = np.delete(pred_list,-1)\n",
    "        gold_list = np.delete(gold_list,-1)\n",
    "            \n",
    "        if torch.cuda.is_available():\n",
    "            train_acc = accuracy_score(gold_list.cpu(), pred_list.cpu())\n",
    "        else:\n",
    "            train_acc = accuracy_score(gold_list, pred_list)\n",
    "\n",
    "        print(\"train_acc :\",train_acc)        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  82. 確率的勾配降下法による学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題81で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "train_acc : 0.6216190833959429\n",
      "train_loss : 334.00431844592094\n",
      "dev_acc : 0.6729041916167665\n",
      "devloss : 37.01860195398331\n",
      "1\n",
      "train_acc : 0.7006010518407213\n",
      "train_loss : 269.1085135638714\n",
      "dev_acc : 0.6916167664670658\n",
      "devloss : 34.09345591068268\n",
      "2\n",
      "train_acc : 0.7271788129226145\n",
      "train_loss : 242.58858534693718\n",
      "dev_acc : 0.7208083832335329\n",
      "devloss : 32.20902916789055\n",
      "3\n",
      "train_acc : 0.7556348610067618\n",
      "train_loss : 221.2155325114727\n",
      "dev_acc : 0.7357784431137725\n",
      "devloss : 30.744750767946243\n",
      "4\n",
      "train_acc : 0.7821187077385424\n",
      "train_loss : 201.87052065134048\n",
      "dev_acc : 0.7410179640718563\n",
      "devloss : 30.107810616493225\n",
      "5\n",
      "train_acc : 0.7982719759579263\n",
      "train_loss : 184.1765981465578\n",
      "dev_acc : 0.7574850299401198\n",
      "devloss : 28.469183325767517\n",
      "6\n",
      "train_acc : 0.818557475582269\n",
      "train_loss : 168.24360464513302\n",
      "dev_acc : 0.7544910179640718\n",
      "devloss : 28.34752196073532\n",
      "7\n",
      "train_acc : 0.8386551465063862\n",
      "train_loss : 153.6498180180788\n",
      "dev_acc : 0.7702095808383234\n",
      "devloss : 27.637164175510406\n",
      "8\n",
      "train_acc : 0.8559353869271225\n",
      "train_loss : 138.86217760294676\n",
      "dev_acc : 0.7836826347305389\n",
      "devloss : 26.85340803861618\n",
      "9\n",
      "train_acc : 0.863824192336589\n",
      "train_loss : 126.70108041912317\n",
      "dev_acc : 0.7836826347305389\n",
      "devloss : 26.5865431278944\n"
     ]
    }
   ],
   "source": [
    "#train dev test 前の章で作ったものを使う\n",
    "#full_data_tokened.txtは3つの全部が入ってる\n",
    "#mozes でtトークナイズ済み\n",
    "import os \n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence,pad_sequence,pack_sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim,hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim,tagset_size)\n",
    "\n",
    "    def forward(self,sentence,word_seq_lengths):\n",
    "        embeds = self.word_embeddings(sentence)  \n",
    "        #手書きでpaddingしたのでpackしてRNNに入れて戻す\n",
    "        packed_words = pack_padded_sequence(embeds, word_seq_lengths.cpu().numpy(), batch_first=True)  \n",
    "        hidden = None\n",
    "        output, _ = self.rnn(packed_words,hidden)\n",
    "        # output, _ = self.rnn(embeds)\n",
    "        output, _ = pad_packed_sequence(output,batch_first=True)\n",
    "        \n",
    "        #ここ0の分処理してないので直す\n",
    "        output=torch.sum(output,dim=1)\n",
    "        \n",
    "        tag_space = self.hidden2tag(output)\n",
    " \n",
    "        tag_scores = F.log_softmax(tag_space,dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "#vocab 次元の辞書を作る 80番の内容\n",
    "def make_vocab(data_path):\n",
    "    vocab = {}\n",
    "    with open(data_path) as data:\n",
    "        for line in data:\n",
    "            text_data = line.split(\"\\t\")[0]\n",
    "            words = text_data.split()\n",
    "\n",
    "            for word in  words:\n",
    "                if word in vocab:\n",
    "                    vocab[word] +=1\n",
    "                else:\n",
    "                    vocab[word] = 1\n",
    "\n",
    "    #(単語,頻度)の順にタプルで入ってる\n",
    "    vocab_sorted = sorted(vocab.items(),key=lambda x:x[1])[::-1]\n",
    "\n",
    "    vocab_with_id = {}\n",
    "    #idに変換\n",
    "    for item in vocab_sorted:\n",
    "        if item[1] >= 2:\n",
    "            vocab_with_id[item[0]] = len(vocab_with_id.items())\n",
    "        else:\n",
    "            vocab_with_id[item[0]] = 0\n",
    "\n",
    "    return vocab_with_id\n",
    "\n",
    "#train,test,devを読み込む関数\n",
    "def data_import(mode,vocab_with_id):\n",
    "    cu_path = os.getcwd()\n",
    "    name = mode\n",
    "    data_path = os.path.join(cu_path,\"data\",name+\".txt\")\n",
    "\n",
    "    text_data_list = []\n",
    "    label_list = []\n",
    "    with open(data_path) as data:\n",
    "        for line in data:\n",
    "            text_data,label  = line.strip().split(\"\\t\")\n",
    "            words = text_data.split()\n",
    "            text_data_list.append(words)\n",
    "            label_list.append(label)\n",
    "    \n",
    "    return text_data_list,label_list\n",
    "\n",
    "#単語列 -> id に変換する\n",
    "def prepare_sequence(seqs,with_id,mode):\n",
    "    bacth_id_list = []\n",
    "    for seq in seqs:\n",
    "        idxs = [with_id[w] for w in seq]\n",
    "        bacth_id_list.append(idxs)\n",
    "    # print(bacth_id_list)\n",
    "    # return torch.tensor(idxs, dtype=torch.long)\n",
    "    return bacth_id_list\n",
    "\n",
    "#結局手で書いてしまった\n",
    "#系列長が違う生でlist > tensor の変換がうまくいかないので0埋めを手動でやった\n",
    "def sequence2padded_tesnsor(seqs,labels):    \n",
    "    batch_size = len(seqs)\n",
    "    word_seq_lengths = torch.LongTensor(list(map(len, seqs)))\n",
    "    max_seq_len = word_seq_lengths.max().item()\n",
    "    word_seq_tensor = torch.zeros((batch_size, max_seq_len), requires_grad=True).long()\n",
    "    label_seq_tensor = torch.zeros((batch_size), requires_grad=True).long()\n",
    "    # mask = torch.zeros((batch_size, max_seq_len), requires_grad=True).byte()\n",
    "    for idx, (seq, label,seqlen) in enumerate(zip(seqs, labels,word_seq_lengths)):\n",
    "        seqlen = seqlen.item()\n",
    "        word_seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "        label_seq_tensor[idx] = torch.LongTensor(label)\n",
    "        # mask[idx, :seqlen] = torch.Tensor([1] * seqlen)\n",
    "    \n",
    "    word_seq_lengths, word_perm_idx = word_seq_lengths.sort(0, descending=True)\n",
    "    word_seq_tensor = word_seq_tensor[word_perm_idx]#長さごとにに並べ替え\n",
    "    label_seq_tensor = label_seq_tensor[word_perm_idx]\n",
    "\n",
    "    return word_seq_tensor,label_seq_tensor,word_seq_lengths\n",
    "    \n",
    "def main():\n",
    "    #paraameter\n",
    "    EMBEDDING_DIM = 300\n",
    "    HIDDEN_DIM = 50\n",
    "    BATCH_SIZE = 32\n",
    "     \n",
    "    cu_path = os.getcwd()\n",
    "    data_path = os.path.join(cu_path,'data','full_data_tokened.txt')\n",
    "    vocab_with_id = make_vocab(data_path)\n",
    "    train_X, train_y = data_import(\"train\",vocab_with_id)\n",
    "\n",
    "    label_with_id = {\"b\":0,\"t\":1,\"e\":2,\"m\":3}\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model = SimpleRNN(EMBEDDING_DIM, \n",
    "                      HIDDEN_DIM, \n",
    "                      len(vocab_with_id), \n",
    "                      len(label_with_id))\n",
    "    loss_function = nn.NLLLoss()\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        print(epoch)\n",
    "        train_num = len(train_X)\n",
    "        batch_size = BATCH_SIZE\n",
    "\n",
    "        #shuffle\n",
    "        combined=list(zip(train_X,train_y))\n",
    "        random.shuffle(combined)\n",
    "        train_X,train_y=zip(*combined)\n",
    "\n",
    "        if train_num % batch_size == 0:\n",
    "            total_batch = train_num // batch_size \n",
    "        else:\n",
    "            total_batch = train_num // batch_size + 1\n",
    "        \n",
    "        loss_total = 0\n",
    "        gold_list = np.array([-1])\n",
    "        pred_list = np.array([-1])\n",
    "        \n",
    "        for batch_id in range(total_batch):\n",
    "            #それぞれのsizeが違ってdata_loaderの使いかたがわからなかった\n",
    "            start = batch_id * batch_size\n",
    "            end = (batch_id + 1) * batch_size\n",
    "            sentences = train_X[start:end]\n",
    "            tags = train_y[start:end]\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            sentence_in = prepare_sequence(sentences, vocab_with_id,\"X\")\n",
    "            targets = prepare_sequence(tags, label_with_id,\"y\")\n",
    "            sentence_in, targets, word_seq_lengths = sequence2padded_tesnsor(sentence_in,targets)\n",
    "\n",
    "            tag_scores = model(sentence_in,word_seq_lengths)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            \n",
    "            # print(tag_scores)\n",
    "            loss_total += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pred_y = torch.argmax(tag_scores,dim=1).numpy()\n",
    "            pred_list = np.insert(pred_list,-1,pred_y)\n",
    "        \n",
    "            gold_y = np.array(targets)\n",
    "            gold_list = np.insert(gold_list,-1,gold_y)\n",
    "            \n",
    "        pred_list = np.delete(pred_list,-1)\n",
    "        gold_list = np.delete(gold_list,-1)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            train_acc = accuracy_score(gold_list.cpu(), pred_list.cpu())\n",
    "        else:\n",
    "            train_acc = accuracy_score(gold_list, pred_list)\n",
    "\n",
    "        print(\"train_acc :\",train_acc)        \n",
    "        print(\"train_loss :\",loss_total)\n",
    "\n",
    "        #========================================================\n",
    "        #dev\n",
    "        dev_X, dev_y = data_import(\"dev\",vocab_with_id)\n",
    "        train_num = len(dev_X)\n",
    "        batch_size = BATCH_SIZE\n",
    "\n",
    "        if train_num % batch_size == 0:\n",
    "            total_batch = train_num // batch_size \n",
    "        else:\n",
    "            total_batch = train_num // batch_size + 1\n",
    "\n",
    "        dev_loss_total = 0\n",
    "        gold_list = np.array([-1])\n",
    "        pred_list = np.array([-1])\n",
    "\n",
    "        model.eval()\n",
    "        for batch_id in range(total_batch):\n",
    "            #それぞれのsizeが違ってdata_loaderの使いかたがわからなかった\n",
    "            start = batch_id * batch_size\n",
    "            end = (batch_id + 1) * batch_size\n",
    "            sentences = dev_X[start:end]\n",
    "            tags = dev_y[start:end]\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            sentence_in = prepare_sequence(sentences, vocab_with_id,\"X\")\n",
    "            targets = prepare_sequence(tags, label_with_id,\"y\")\n",
    "            sentence_in, targets, word_seq_lengths = sequence2padded_tesnsor(sentence_in,targets)\n",
    "\n",
    "            tag_scores = model(sentence_in,word_seq_lengths)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            \n",
    "            # print(tag_scores)\n",
    "            dev_loss_total += loss.item()\n",
    "\n",
    "            pred_y = torch.argmax(tag_scores,dim=1).numpy()\n",
    "            pred_list = np.insert(pred_list,-1,pred_y)\n",
    "        \n",
    "            gold_y = np.array(targets)\n",
    "            gold_list = np.insert(gold_list,-1,gold_y)\n",
    "            \n",
    "        pred_list = np.delete(pred_list,-1)\n",
    "        gold_list = np.delete(gold_list,-1)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            dev_acc = accuracy_score(gold_list.cpu(), pred_list.cpu())\n",
    "        else:\n",
    "            dev_acc = accuracy_score(gold_list, pred_list)\n",
    "\n",
    "        print(\"dev_acc :\",dev_acc)        \n",
    "        print(\"devloss :\",dev_loss_total)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 83. ミニバッチ化・GPU上での学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問題82のコードを改変し，B\n",
    "事例ごとに損失・勾配を計算して学習を行えるようにせよ（B\n",
    "の値は適当に選べ）．また，GPU上で学習を実行せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "train_acc : 0.6132607062359129\n",
      "train_loss : 334.6142292022705\n",
      "dev_acc : 0.6796407185628742\n",
      "devloss : 36.24382966756821\n",
      "1\n",
      "train_acc : 0.697877535687453\n",
      "train_loss : 269.3880696296692\n",
      "dev_acc : 0.7050898203592815\n",
      "devloss : 32.87444168329239\n",
      "2\n",
      "train_acc : 0.7277422990232908\n",
      "train_loss : 241.03529497981071\n",
      "dev_acc : 0.7215568862275449\n",
      "devloss : 31.17666858434677\n",
      "3\n",
      "train_acc : 0.7537565740045079\n",
      "train_loss : 218.00044417381287\n",
      "dev_acc : 0.7342814371257484\n",
      "devloss : 30.350026458501816\n",
      "4\n",
      "train_acc : 0.7831517655897822\n",
      "train_loss : 198.1716799288988\n",
      "dev_acc : 0.7402694610778443\n",
      "devloss : 28.892420887947083\n",
      "5\n",
      "train_acc : 0.8026859504132231\n",
      "train_loss : 181.21855601668358\n",
      "dev_acc : 0.7470059880239521\n",
      "devloss : 27.663793861865997\n",
      "6\n",
      "train_acc : 0.8218444778362134\n",
      "train_loss : 165.22911374270916\n",
      "dev_acc : 0.7604790419161677\n",
      "devloss : 27.004014551639557\n",
      "7\n",
      "train_acc : 0.8385612321562734\n",
      "train_loss : 150.19089603424072\n",
      "dev_acc : 0.7612275449101796\n",
      "devloss : 26.666532933712006\n",
      "8\n",
      "train_acc : 0.8539631855747558\n",
      "train_loss : 136.44484202563763\n",
      "dev_acc : 0.7896706586826348\n",
      "devloss : 25.775493651628494\n",
      "9\n",
      "train_acc : 0.8684259954921112\n",
      "train_loss : 123.25785817205906\n",
      "dev_acc : 0.7799401197604791\n",
      "devloss : 25.448445230722427\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence,pad_sequence,pack_sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim,hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim,tagset_size)\n",
    "\n",
    "    def forward(self,sentence,word_seq_lengths):\n",
    "        embeds = self.word_embeddings(sentence)  \n",
    "        #手書きでpaddingしたのでpackしてRNNに入れて戻す\n",
    "        packed_words = pack_padded_sequence(embeds, word_seq_lengths.cpu().numpy(), batch_first=True)   \n",
    "        hidden = None\n",
    "        output, _ = self.rnn(packed_words,hidden)\n",
    "        # output, _ = self.rnn(embeds)\n",
    "        output, _ = pad_packed_sequence(output,batch_first=True)\n",
    "        \n",
    "        #ここ0の分処理してないので直す\n",
    "        output=torch.sum(output,dim=1)\n",
    "        \n",
    "        tag_space = self.hidden2tag(output)\n",
    " \n",
    "        tag_scores = F.log_softmax(tag_space,dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "#vocab 次元の辞書を作る 80番の内容\n",
    "def make_vocab(data_path):\n",
    "    vocab = {}\n",
    "    with open(data_path) as data:\n",
    "        for line in data:\n",
    "            text_data = line.split(\"\\t\")[0]\n",
    "            words = text_data.split()\n",
    "\n",
    "            for word in  words:\n",
    "                if word in vocab:\n",
    "                    vocab[word] +=1\n",
    "                else:\n",
    "                    vocab[word] = 1\n",
    "\n",
    "    #(単語,頻度)の順にタプルで入ってる\n",
    "    vocab_sorted = sorted(vocab.items(),key=lambda x:x[1])[::-1]\n",
    "\n",
    "    vocab_with_id = {}\n",
    "    #idに変換\n",
    "    for item in vocab_sorted:\n",
    "        if item[1] >= 2:\n",
    "            vocab_with_id[item[0]] = len(vocab_with_id.items())\n",
    "        else:\n",
    "            vocab_with_id[item[0]] = 0\n",
    "\n",
    "    return vocab_with_id\n",
    "\n",
    "#train,test,devを読み込む関数\n",
    "def data_import(mode,vocab_with_id):\n",
    "    cu_path = os.getcwd()\n",
    "    name = mode\n",
    "    data_path = os.path.join(cu_path,\"data\",name+\".txt\")\n",
    "\n",
    "    text_data_list = []\n",
    "    label_list = []\n",
    "    with open(data_path) as data:\n",
    "        for line in data:\n",
    "            text_data,label  = line.strip().split(\"\\t\")\n",
    "            words = text_data.split()\n",
    "            text_data_list.append(words)\n",
    "            label_list.append(label)\n",
    "    \n",
    "    return text_data_list,label_list\n",
    "\n",
    "#単語列 -> id に変換する\n",
    "def prepare_sequence(seqs,with_id,mode):\n",
    "    bacth_id_list = []\n",
    "    for seq in seqs:\n",
    "        idxs = [with_id[w] for w in seq]\n",
    "        bacth_id_list.append(idxs)\n",
    "    # print(bacth_id_list)\n",
    "    # return torch.tensor(idxs, dtype=torch.long)\n",
    "    return bacth_id_list\n",
    "\n",
    "#結局手で書いてしまった\n",
    "#系列長が違う生でlist > tensor の変換がうまくいかないので0埋めを手動でやった\n",
    "def sequence2padded_tesnsor(seqs,labels,device):    \n",
    "    batch_size = len(seqs)\n",
    "    word_seq_lengths = torch.LongTensor(list(map(len, seqs)))\n",
    "    max_seq_len = word_seq_lengths.max().item()\n",
    "    word_seq_tensor = torch.zeros((batch_size, max_seq_len), requires_grad=True).long()\n",
    "    label_seq_tensor = torch.zeros((batch_size), requires_grad=True).long()\n",
    "    # mask = torch.zeros((batch_size, max_seq_len), requires_grad=True).byte()\n",
    "    for idx, (seq, label,seqlen) in enumerate(zip(seqs, labels,word_seq_lengths)):\n",
    "        seqlen = seqlen.item()\n",
    "        word_seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "        label_seq_tensor[idx] = torch.LongTensor(label)\n",
    "        # mask[idx, :seqlen] = torch.Tensor([1] * seqlen)\n",
    "    \n",
    "    word_seq_lengths, word_perm_idx = word_seq_lengths.sort(0, descending=True)\n",
    "    word_seq_tensor = word_seq_tensor[word_perm_idx]#長さごとにに並べ替え\n",
    "    label_seq_tensor = label_seq_tensor[word_perm_idx]\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        word_seq_tensor = word_seq_tensor.to(device)\n",
    "        label_seq_tensor = label_seq_tensor.to(device)\n",
    "        word_seq_lengths = word_seq_lengths.to(device)\n",
    "\n",
    "    return word_seq_tensor,label_seq_tensor,word_seq_lengths\n",
    "    \n",
    "def main():\n",
    "    #paraameter\n",
    "    EMBEDDING_DIM = 300\n",
    "    HIDDEN_DIM = 50\n",
    "    BATCH_SIZE = 32\n",
    "     \n",
    "    cu_path = os.getcwd()\n",
    "    data_path = os.path.join(cu_path,'data','full_data_tokened.txt')\n",
    "    vocab_with_id = make_vocab(data_path)\n",
    "    train_X, train_y = data_import(\"train\",vocab_with_id)\n",
    "\n",
    "    label_with_id = {\"b\":0,\"t\":1,\"e\":2,\"m\":3}\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model = SimpleRNN(EMBEDDING_DIM, \n",
    "                      HIDDEN_DIM, \n",
    "                      len(vocab_with_id), \n",
    "                      len(label_with_id))\n",
    "    loss_function = nn.NLLLoss()\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        print(epoch)\n",
    "        train_num = len(train_X)\n",
    "        batch_size = BATCH_SIZE\n",
    "\n",
    "        #shuffle\n",
    "        combined=list(zip(train_X,train_y))\n",
    "        random.shuffle(combined)\n",
    "        train_X,train_y=zip(*combined)\n",
    "\n",
    "        if train_num % batch_size == 0:\n",
    "            total_batch = train_num // batch_size \n",
    "        else:\n",
    "            total_batch = train_num // batch_size + 1\n",
    "        \n",
    "        loss_total = 0\n",
    "        gold_list = np.array([-1])\n",
    "        pred_list = np.array([-1])\n",
    "        \n",
    "        for batch_id in range(total_batch):\n",
    "            #それぞれのsizeが違ってdata_loaderの使いかたがわからなかった\n",
    "            start = batch_id * batch_size\n",
    "            end = (batch_id + 1) * batch_size\n",
    "            sentences = train_X[start:end]\n",
    "            tags = train_y[start:end]\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            sentence_in = prepare_sequence(sentences, vocab_with_id,\"X\")\n",
    "            targets = prepare_sequence(tags, label_with_id,\"y\")\n",
    "            sentence_in, targets, word_seq_lengths = sequence2padded_tesnsor(sentence_in,targets,device)\n",
    "\n",
    "            tag_scores = model(sentence_in,word_seq_lengths)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            \n",
    "            # print(tag_scores)\n",
    "            loss_total += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                pred_y = torch.argmax(tag_scores,dim=1).cpu().numpy()\n",
    "            else:\n",
    "                pred_y = torch.argmax(tag_scores,dim=1).numpy()\n",
    "            pred_list = np.insert(pred_list,-1,pred_y)\n",
    "        \n",
    "            gold_y = np.array(targets.cpu())\n",
    "            gold_list = np.insert(gold_list,-1,gold_y)\n",
    "            \n",
    "        pred_list = np.delete(pred_list,-1)\n",
    "        gold_list = np.delete(gold_list,-1)\n",
    "        \n",
    "        train_acc = accuracy_score(gold_list, pred_list)\n",
    "\n",
    "        print(\"train_acc :\",train_acc)        \n",
    "        print(\"train_loss :\",loss_total)\n",
    "\n",
    "        #========================================================\n",
    "        #dev\n",
    "        dev_X, dev_y = data_import(\"dev\",vocab_with_id)\n",
    "        train_num = len(dev_X)\n",
    "        batch_size = BATCH_SIZE\n",
    "\n",
    "        if train_num % batch_size == 0:\n",
    "            total_batch = train_num // batch_size \n",
    "        else:\n",
    "            total_batch = train_num // batch_size + 1\n",
    "\n",
    "        dev_loss_total = 0\n",
    "        gold_list = np.array([-1])\n",
    "        pred_list = np.array([-1])\n",
    "\n",
    "        model.eval()\n",
    "        for batch_id in range(total_batch):\n",
    "            #それぞれのsizeが違ってdata_loaderの使いかたがわからなかった\n",
    "            start = batch_id * batch_size\n",
    "            end = (batch_id + 1) * batch_size\n",
    "            sentences = dev_X[start:end]\n",
    "            tags = dev_y[start:end]\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            sentence_in = prepare_sequence(sentences, vocab_with_id,\"X\")\n",
    "            targets = prepare_sequence(tags, label_with_id,\"y\")\n",
    "            sentence_in, targets, word_seq_lengths = sequence2padded_tesnsor(sentence_in,targets,device)\n",
    "\n",
    "            tag_scores = model(sentence_in,word_seq_lengths)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            \n",
    "            # print(tag_scores)\n",
    "            dev_loss_total += loss.item()\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                pred_y = torch.argmax(tag_scores,dim=1).cpu().numpy()\n",
    "            else:\n",
    "                pred_y = torch.argmax(tag_scores,dim=1).numpy()\n",
    "            pred_list = np.insert(pred_list,-1,pred_y)\n",
    "        \n",
    "            gold_y = np.array(targets.cpu())\n",
    "            gold_list = np.insert(gold_list,-1,gold_y)\n",
    "            \n",
    "        pred_list = np.delete(pred_list,-1)\n",
    "        gold_list = np.delete(gold_list,-1)\n",
    "        \n",
    "        dev_acc = accuracy_score(gold_list, pred_list)\n",
    "\n",
    "        print(\"dev_acc :\",dev_acc)        \n",
    "        print(\"devloss :\",dev_loss_total)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 84. 単語ベクトルの導入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事前学習済みの単語ベクトル（例えば，Google Newsデータセット（約1,000億単語）での学習済み単語ベクトル）で単語埋め込みemb(x)\n",
    "を初期化し，学習せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "train_acc : 0.7383546205860255\n",
      "train_loss : 242.85468243062496\n",
      "dev_acc : 0.8038922155688623\n",
      "devloss : 22.48978614807129\n",
      "1\n",
      "train_acc : 0.8225957926371149\n",
      "train_loss : 167.74377293139696\n",
      "dev_acc : 0.8473053892215568\n",
      "devloss : 18.099978432059288\n",
      "2\n",
      "train_acc : 0.8524605559729527\n",
      "train_loss : 141.10449175536633\n",
      "dev_acc : 0.8637724550898204\n",
      "devloss : 16.390637129545212\n",
      "3\n",
      "train_acc : 0.8672051089406462\n",
      "train_loss : 131.88817410171032\n",
      "dev_acc : 0.8510479041916168\n",
      "devloss : 17.1375330388546\n",
      "4\n",
      "train_acc : 0.8752817430503381\n",
      "train_loss : 122.67428221926093\n",
      "dev_acc : 0.7305389221556886\n",
      "devloss : 31.017480820417404\n",
      "5\n",
      "train_acc : 0.8855184072126221\n",
      "train_loss : 112.81891445443034\n",
      "dev_acc : 0.8308383233532934\n",
      "devloss : 20.215745612978935\n",
      "6\n",
      "train_acc : 0.902610818933133\n",
      "train_loss : 98.28194688819349\n",
      "dev_acc : 0.8817365269461078\n",
      "devloss : 14.844500876963139\n",
      "7\n",
      "train_acc : 0.8713373403456048\n",
      "train_loss : 126.15577337145805\n",
      "dev_acc : 0.8540419161676647\n",
      "devloss : 17.645660877227783\n",
      "8\n",
      "train_acc : 0.9055221637866266\n",
      "train_loss : 96.2272194288671\n",
      "dev_acc : 0.8727544910179641\n",
      "devloss : 16.749839462339878\n",
      "9\n",
      "train_acc : 0.922145003756574\n",
      "train_loss : 76.35082922317088\n",
      "dev_acc : 0.8690119760479041\n",
      "devloss : 17.168788850307465\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence,pad_sequence,pack_sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "import gensim\n",
    "\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size,pre_trained_embedding):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.word_embeddings.weight = nn.Parameter(torch.from_numpy(pre_trained_embedding))\n",
    "        self.rnn = nn.RNN(embedding_dim,hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim,tagset_size)\n",
    "\n",
    "        # if pre_trained_embedding:\n",
    "\n",
    "\n",
    "    def forward(self,sentence,word_seq_lengths):\n",
    "        embeds = self.word_embeddings(sentence)  \n",
    "        #手書きでpaddingしたのでpackしてRNNに入れて戻す\n",
    "        packed_words = pack_padded_sequence(embeds, word_seq_lengths.cpu().numpy(), batch_first=True) \n",
    "        hidden = None\n",
    "        output, h_n = self.rnn(packed_words,hidden)\n",
    "        #h_nは各時刻の情報がpackされている情報がくる\n",
    "        #そのまま使うとpackの最後の情報がピンポイントで使えて便利\n",
    "        tag_space = self.hidden2tag(h_n[0])\n",
    " \n",
    "        tag_scores = F.log_softmax(tag_space,dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "#vocab 次元の辞書を作る 80番の内容\n",
    "def make_vocab(data_path):\n",
    "    vocab = {}\n",
    "    with open(data_path) as data:\n",
    "        for line in data:\n",
    "            text_data = line.split(\"\\t\")[0]\n",
    "            words = text_data.split()\n",
    "\n",
    "            for word in  words:\n",
    "                if word in vocab:\n",
    "                    vocab[word] +=1\n",
    "                else:\n",
    "                    vocab[word] = 1\n",
    "\n",
    "    #(単語,頻度)の順にタプルで入ってる\n",
    "    vocab_sorted = sorted(vocab.items(),key=lambda x:x[1])[::-1]\n",
    "\n",
    "    vocab_with_id = {}\n",
    "    #idに変換\n",
    "    for item in vocab_sorted:\n",
    "        #default_dictを使って一行で書く\n",
    "        if item[1] >= 2:\n",
    "            vocab_with_id[item[0]] = len(vocab_with_id.items())+1\n",
    "        else:\n",
    "            vocab_with_id[item[0]] = 0\n",
    "\n",
    "    return vocab_with_id\n",
    "\n",
    "#train,test,devを読み込む関数\n",
    "def data_import(mode,vocab_with_id):\n",
    "    cu_path = os.getcwd()\n",
    "    name = mode\n",
    "    data_path = os.path.join(cu_path,\"data\",name+\".txt\")\n",
    "\n",
    "    text_data_list = []\n",
    "    label_list = []\n",
    "    with open(data_path) as data:\n",
    "        for line in data:\n",
    "            text_data,label  = line.strip().split(\"\\t\")\n",
    "            words = text_data.split()\n",
    "            text_data_list.append(words)\n",
    "            label_list.append(label)\n",
    "    \n",
    "    return text_data_list,label_list\n",
    "\n",
    "#単語列 -> id に変換する\n",
    "#ここでID errorの時に0に処理するプログラムに変える\n",
    "def prepare_sequence(seqs,with_id,mode):\n",
    "    bacth_id_list = []\n",
    "    for seq in seqs:\n",
    "        idxs = [with_id[w] if (w in with_id) else 0 for w in seq]\n",
    "        bacth_id_list.append(idxs)\n",
    "    # print(bacth_id_list)\n",
    "    # return torch.tensor(idxs, dtype=torch.long)\n",
    "    return bacth_id_list\n",
    "\n",
    "\n",
    "#torch.saveはnumpyでもできる\n",
    "#data => torch.saveでsave それをtorch.loadで読み込むと早くなるかも\n",
    "\n",
    "#データセットをある程度長さごとに固めてバッチを作ると系列長が揃って処理時間が短くなる\n",
    "#Allen_NLPを使うと便利\n",
    "\n",
    "#結局手で書いてしまった\n",
    "#系列長が違う生でlist > tensor の変換がうまくいかないので0埋めを手動でやった\n",
    "def sequence2padded_tesnsor(seqs,labels,device):    \n",
    "    batch_size = len(seqs)\n",
    "    word_seq_lengths = torch.LongTensor(list(map(len, seqs)))\n",
    "    max_seq_len = word_seq_lengths.max().item()\n",
    "    word_seq_tensor = torch.zeros((batch_size, max_seq_len), requires_grad=True).long()\n",
    "    label_seq_tensor = torch.zeros((batch_size), requires_grad=True).long()\n",
    "    # mask = torch.zeros((batch_size, max_seq_len), requires_grad=True).byte()\n",
    "    for idx, (seq, label,seqlen) in enumerate(zip(seqs, labels,word_seq_lengths)):\n",
    "        seqlen = seqlen.item()\n",
    "        word_seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "        label_seq_tensor[idx] = torch.LongTensor(label)\n",
    "        # mask[idx, :seqlen] = torch.Tensor([1] * seqlen)\n",
    "    \n",
    "    word_seq_lengths, word_perm_idx = word_seq_lengths.sort(0, descending=True)\n",
    "    word_seq_tensor = word_seq_tensor[word_perm_idx]#長さごとにに並べ替え\n",
    "    label_seq_tensor = label_seq_tensor[word_perm_idx]\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        word_seq_tensor = word_seq_tensor.to(device)\n",
    "        label_seq_tensor = label_seq_tensor.to(device)\n",
    "        word_seq_lengths = word_seq_lengths.to(device)\n",
    "\n",
    "    return word_seq_tensor,label_seq_tensor,word_seq_lengths\n",
    "\n",
    "def init_embedding(vocab_with_id,EMBEDDING_DIM):\n",
    "    from gensim.models import KeyedVectors\n",
    "    path = 'data/GoogleNews-vectors-negative300.bin'\n",
    "    vectors = KeyedVectors.load_word2vec_format(path,binary=True)\n",
    "    matrix = np.zeros((len(vocab_with_id),EMBEDDING_DIM))\n",
    "\n",
    "    for word,id in vocab_with_id.items():\n",
    "        if word in vectors.vocab:\n",
    "            matrix[id] = vectors[word]\n",
    "    matrix[0] = np.zeros(EMBEDDING_DIM)\n",
    "    \n",
    "\n",
    "    return matrix\n",
    "        \n",
    "\n",
    "def main():\n",
    "    #paraameter\n",
    "    EMBEDDING_DIM = 300\n",
    "    HIDDEN_DIM = 50\n",
    "    BATCH_SIZE = 32\n",
    "     \n",
    "    cu_path = os.getcwd()\n",
    "    data_path = os.path.join(cu_path,'data','train.txt')\n",
    "    vocab_with_id = make_vocab(data_path)\n",
    "    pre_trained_embedding = init_embedding(vocab_with_id,EMBEDDING_DIM)\n",
    "    \n",
    "    train_X, train_y = data_import(\"train\",vocab_with_id)\n",
    "\n",
    "    label_with_id = {\"b\":0,\"t\":1,\"e\":2,\"m\":3}\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model = SimpleRNN(EMBEDDING_DIM, \n",
    "                      HIDDEN_DIM, \n",
    "                      len(vocab_with_id), \n",
    "                      len(label_with_id),\n",
    "                      pre_trained_embedding).float()\n",
    "    #型エラーが出たのでfloatにして修正\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        print(epoch)\n",
    "        train_num = len(train_X)\n",
    "        batch_size = BATCH_SIZE\n",
    "\n",
    "        #shuffle\n",
    "        combined=list(zip(train_X,train_y))\n",
    "        random.shuffle(combined)\n",
    "        train_X,train_y=zip(*combined)\n",
    "\n",
    "        if train_num % batch_size == 0:\n",
    "            total_batch = train_num // batch_size \n",
    "        else:\n",
    "            total_batch = train_num // batch_size + 1\n",
    "        \n",
    "        loss_total = 0\n",
    "        gold_list = np.array([-1])\n",
    "        pred_list = np.array([-1])\n",
    "        \n",
    "        for batch_id in range(total_batch):\n",
    "            #それぞれのsizeが違ってdata_loaderの使いかたがわからなかった\n",
    "            start = batch_id * batch_size\n",
    "            end = (batch_id + 1) * batch_size\n",
    "            sentences = train_X[start:end]\n",
    "            tags = train_y[start:end]\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            sentence_in = prepare_sequence(sentences, vocab_with_id,\"X\")\n",
    "            targets = prepare_sequence(tags, label_with_id,\"y\")\n",
    "            sentence_in, targets, word_seq_lengths = sequence2padded_tesnsor(sentence_in,targets,device)\n",
    "\n",
    "            tag_scores = model(sentence_in,word_seq_lengths)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            \n",
    "            # print(tag_scores)\n",
    "            loss_total += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                pred_y = torch.argmax(tag_scores,dim=1).cpu().numpy()\n",
    "            else:\n",
    "                pred_y = torch.argmax(tag_scores,dim=1).numpy()\n",
    "            pred_list = np.insert(pred_list,-1,pred_y)\n",
    "        \n",
    "            gold_y = np.array(targets.cpu())\n",
    "            gold_list = np.insert(gold_list,-1,gold_y)\n",
    "            \n",
    "        pred_list = np.delete(pred_list,-1)\n",
    "        gold_list = np.delete(gold_list,-1)\n",
    "        \n",
    "        train_acc = accuracy_score(gold_list, pred_list)\n",
    "\n",
    "        print(\"train_acc :\",train_acc)        \n",
    "        print(\"train_loss :\",loss_total)\n",
    "\n",
    "        #========================================================\n",
    "        #dev\n",
    "        dev_X, dev_y = data_import(\"dev\",vocab_with_id)\n",
    "        train_num = len(dev_X)\n",
    "        batch_size = BATCH_SIZE\n",
    "\n",
    "        if train_num % batch_size == 0:\n",
    "            total_batch = train_num // batch_size \n",
    "        else:\n",
    "            total_batch = train_num // batch_size + 1\n",
    "\n",
    "        dev_loss_total = 0\n",
    "        gold_list = np.array([-1])\n",
    "        pred_list = np.array([-1])\n",
    "\n",
    "        model.eval()\n",
    "        for batch_id in range(total_batch):\n",
    "            #それぞれのsizeが違ってdata_loaderの使いかたがわからなかった\n",
    "            start = batch_id * batch_size\n",
    "            end = (batch_id + 1) * batch_size\n",
    "            sentences = dev_X[start:end]\n",
    "            tags = dev_y[start:end]\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            sentence_in = prepare_sequence(sentences, vocab_with_id,\"X\")\n",
    "            targets = prepare_sequence(tags, label_with_id,\"y\")\n",
    "            sentence_in, targets, word_seq_lengths = sequence2padded_tesnsor(sentence_in,targets,device)\n",
    "\n",
    "            tag_scores = model(sentence_in,word_seq_lengths)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            \n",
    "            # print(tag_scores)\n",
    "            dev_loss_total += loss.item()\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                pred_y = torch.argmax(tag_scores,dim=1).cpu().numpy()\n",
    "            else:\n",
    "                pred_y = torch.argmax(tag_scores,dim=1).numpy()\n",
    "            pred_list = np.insert(pred_list,-1,pred_y)\n",
    "        \n",
    "            gold_y = np.array(targets.cpu())\n",
    "            gold_list = np.insert(gold_list,-1,gold_y)\n",
    "            \n",
    "        pred_list = np.delete(pred_list,-1)\n",
    "        gold_list = np.delete(gold_list,-1)\n",
    "        \n",
    "        dev_acc = accuracy_score(gold_list, pred_list)\n",
    "\n",
    "        print(\"dev_acc :\",dev_acc)        \n",
    "        print(\"devloss :\",dev_loss_total)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 85. 双方向RNN・多層化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "順方向と逆方向のRNNの両方を用いて入力テキストをエンコードし，モデルを学習せよ．\n",
    "\n",
    "h⃖ T+1=0,h⃖ t=RNN←−−−(emb(xt),h⃖ t+1),y=softmax(W(yh)[h→T;h⃖ 1]+b(y))\n",
    "ただし，h→t∈ℝdh,h⃖ t∈ℝdh\n",
    "はそれぞれ，順方向および逆方向のRNNで求めた時刻t\n",
    "の隠れ状態ベクトル，RNN←−−−(x,h)\n",
    "は入力x\n",
    "と次時刻の隠れ状態h\n",
    "から前状態を計算するRNNユニット，W(yh)∈ℝL×2dh\n",
    "は隠れ状態ベクトルからカテゴリを予測するための行列，b(y)∈ℝL\n",
    "はバイアス項である．また，[a;b]\n",
    "はベクトルa\n",
    "とb\n",
    "の連結を表す。\n",
    "\n",
    "さらに，双方向RNNを多層化して実験せよ．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "train_acc : 0.7742299023290758\n",
      "train_loss : 205.08350579440594\n",
      "dev_acc : 0.8600299401197605\n",
      "devloss : 17.4151853621006\n",
      "1\n",
      "train_acc : 0.8552779864763336\n",
      "train_loss : 135.75032676011324\n",
      "dev_acc : 0.843562874251497\n",
      "devloss : 17.769000574946404\n",
      "2\n",
      "train_acc : 0.8909654395191585\n",
      "train_loss : 102.9591537117958\n",
      "dev_acc : 0.8607784431137725\n",
      "devloss : 15.9034653455019\n",
      "3\n",
      "train_acc : 0.9167918858001503\n",
      "train_loss : 77.89074492640793\n",
      "dev_acc : 0.8697604790419161\n",
      "devloss : 15.179561972618103\n",
      "4\n",
      "train_acc : 0.935762584522915\n",
      "train_loss : 62.71002884674817\n",
      "dev_acc : 0.8630239520958084\n",
      "devloss : 18.299617916345596\n",
      "5\n",
      "train_acc : 0.9504132231404959\n",
      "train_loss : 48.66107478446793\n",
      "dev_acc : 0.8203592814371258\n",
      "devloss : 26.735527724027634\n",
      "6\n",
      "train_acc : 0.9607438016528925\n",
      "train_loss : 35.55458526988514\n",
      "dev_acc : 0.8540419161676647\n",
      "devloss : 25.607435926795006\n",
      "7\n",
      "train_acc : 0.9572689706987227\n",
      "train_loss : 40.36867264145985\n",
      "dev_acc : 0.8592814371257484\n",
      "devloss : 22.24944542348385\n",
      "8\n",
      "train_acc : 0.9750187828700225\n",
      "train_loss : 24.84984439931577\n",
      "dev_acc : 0.8652694610778443\n",
      "devloss : 21.83910147845745\n",
      "9\n",
      "train_acc : 0.9875093914350113\n",
      "train_loss : 12.656347774725873\n",
      "dev_acc : 0.7911676646706587\n",
      "devloss : 37.79982578754425\n"
     ]
    }
   ],
   "source": [
    "#train dev test 前の章で作ったものを使う\n",
    "#full_data_tokened.txtは3つの全部が入ってる\n",
    "#mozes でtトークナイズ済み\n",
    "import os \n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence,pad_sequence,pack_sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "import gensim\n",
    "\n",
    "\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size,pre_trained_embedding):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        middle_dim = 200\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.word_embeddings.weight = nn.Parameter(torch.from_numpy(pre_trained_embedding))\n",
    "        self.birnn = nn.RNN(embedding_dim,hidden_dim,bidirectional=True)\n",
    "        self.hidden2middle = nn.Linear(hidden_dim*2,middle_dim)\n",
    "        self.middle2tag = nn.Linear(middle_dim,tagset_size)\n",
    "\n",
    "\n",
    "    def forward(self,sentence,word_seq_lengths):\n",
    "        embeds = self.word_embeddings(sentence)  \n",
    "        #手書きでpaddingしたのでpackしてRNNに入れて戻す\n",
    "        packed_words = pack_padded_sequence(embeds, word_seq_lengths.cpu().numpy(), batch_first=True) \n",
    "        hidden = None\n",
    "        output, h_n = self.birnn(packed_words,hidden)\n",
    "        forward_h = h_n[:][0]#前向きの最後\n",
    "        back_h = h_n[:][1]#後ろ向きの最後\n",
    "        birnn_out = torch.cat((forward_h,back_h),1)#catしてhidden_dim*2次元\n",
    "        #h_nは各時刻の情報がpackされている情報がくる\n",
    "        #そのまま使うとpackの最後の情報がピンポイントで使えて便利\n",
    "        middle_out  = self.hidden2middle(birnn_out)\n",
    "        middle_out = F.relu(middle_out)\n",
    "        tag_space = self.middle2tag(middle_out)\n",
    "        tag_scores = F.log_softmax(tag_space,dim=1)\n",
    "\n",
    "        return tag_scores\n",
    "\n",
    "#vocab 次元の辞書を作る 80番の内容\n",
    "def make_vocab(data_path):\n",
    "    vocab = {}\n",
    "    with open(data_path) as data:\n",
    "        for line in data:\n",
    "            text_data = line.split(\"\\t\")[0]\n",
    "            words = text_data.split()\n",
    "\n",
    "            for word in  words:\n",
    "                if word in vocab:\n",
    "                    vocab[word] +=1\n",
    "                else:\n",
    "                    vocab[word] = 1\n",
    "\n",
    "    #(単語,頻度)の順にタプルで入ってる\n",
    "    vocab_sorted = sorted(vocab.items(),key=lambda x:x[1])[::-1]\n",
    "\n",
    "    vocab_with_id = {}\n",
    "    #idに変換\n",
    "    for item in vocab_sorted:\n",
    "        #default_dictを使って一行で書く\n",
    "        if item[1] >= 2:\n",
    "            vocab_with_id[item[0]] = len(vocab_with_id.items())+1\n",
    "        else:\n",
    "            vocab_with_id[item[0]] = 0\n",
    "\n",
    "    return vocab_with_id\n",
    "\n",
    "#train,test,devを読み込む関数\n",
    "def data_import(mode,vocab_with_id):\n",
    "    cu_path = os.getcwd()\n",
    "    name = mode\n",
    "    data_path = os.path.join(cu_path,\"data\",name+\".txt\")\n",
    "\n",
    "    text_data_list = []\n",
    "    label_list = []\n",
    "    with open(data_path) as data:\n",
    "        for line in data:\n",
    "            text_data,label  = line.strip().split(\"\\t\")\n",
    "            words = text_data.split()\n",
    "            text_data_list.append(words)\n",
    "            label_list.append(label)\n",
    "    \n",
    "    return text_data_list,label_list\n",
    "\n",
    "#単語列 -> id に変換する\n",
    "#ここでID errorの時に0に処理するプログラムに変える\n",
    "def prepare_sequence(seqs,with_id,mode):\n",
    "    bacth_id_list = []\n",
    "    for seq in seqs:\n",
    "        idxs = [with_id[w] if (w in with_id) else 0 for w in seq]\n",
    "        bacth_id_list.append(idxs)\n",
    "    # print(bacth_id_list)\n",
    "    # return torch.tensor(idxs, dtype=torch.long)\n",
    "    return bacth_id_list\n",
    "\n",
    "\n",
    "#torch.saveはnumpyでもできる\n",
    "#data => torch.saveでsave それをtorch.loadで読み込むと早くなるかも\n",
    "\n",
    "#データセットをある程度長さごとに固めてバッチを作ると系列長が揃って処理時間が短くなる\n",
    "#Allen_NLPを使うと便利\n",
    "\n",
    "#結局手で書いてしまった\n",
    "#系列長が違う生でlist > tensor の変換がうまくいかないので0埋めを手動でやった\n",
    "def sequence2padded_tesnsor(seqs,labels,device):    \n",
    "    batch_size = len(seqs)\n",
    "    word_seq_lengths = torch.LongTensor(list(map(len, seqs)))\n",
    "    max_seq_len = word_seq_lengths.max().item()\n",
    "    word_seq_tensor = torch.zeros((batch_size, max_seq_len), requires_grad=True).long()\n",
    "    label_seq_tensor = torch.zeros((batch_size), requires_grad=True).long()\n",
    "    # mask = torch.zeros((batch_size, max_seq_len), requires_grad=True).byte()\n",
    "    for idx, (seq, label,seqlen) in enumerate(zip(seqs, labels,word_seq_lengths)):\n",
    "        seqlen = seqlen.item()\n",
    "        word_seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "        label_seq_tensor[idx] = torch.LongTensor(label)\n",
    "        # mask[idx, :seqlen] = torch.Tensor([1] * seqlen)\n",
    "    \n",
    "    word_seq_lengths, word_perm_idx = word_seq_lengths.sort(0, descending=True)\n",
    "    word_seq_tensor = word_seq_tensor[word_perm_idx]#長さごとにに並べ替え\n",
    "    label_seq_tensor = label_seq_tensor[word_perm_idx]\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        word_seq_tensor = word_seq_tensor.to(device)\n",
    "        label_seq_tensor = label_seq_tensor.to(device)\n",
    "        word_seq_lengths = word_seq_lengths.to(device)\n",
    "\n",
    "    return word_seq_tensor,label_seq_tensor,word_seq_lengths\n",
    "\n",
    "def init_embedding(vocab_with_id,EMBEDDING_DIM):\n",
    "    from gensim.models import KeyedVectors\n",
    "    #作ったvocabのindexに紐付ける\n",
    "    matrix = np.zeros((len(vocab_with_id),EMBEDDING_DIM))\n",
    "    path = 'data/GoogleNews-vectors-negative300.bin'\n",
    "    vectors = KeyedVectors.load_word2vec_format(path,binary=True)\n",
    "    for word,id in vocab_with_id.items():\n",
    "        if word in vectors.vocab:\n",
    "            matrix[id] = vectors[word]\n",
    "\n",
    "    matrix[0] = np.zeros(EMBEDDING_DIM)\n",
    "    \n",
    "    return matrix\n",
    "        \n",
    "def main():\n",
    "    #paraameter\n",
    "    EMBEDDING_DIM = 300\n",
    "    HIDDEN_DIM = 50\n",
    "    BATCH_SIZE = 32\n",
    "     \n",
    "    cu_path = os.getcwd()\n",
    "    data_path = os.path.join(cu_path,'data','train.txt')\n",
    "    vocab_with_id = make_vocab(data_path)\n",
    "    pre_trained_embedding = init_embedding(vocab_with_id,EMBEDDING_DIM)\n",
    "    \n",
    "    train_X, train_y = data_import(\"train\",vocab_with_id)\n",
    "\n",
    "    label_with_id = {\"b\":0,\"t\":1,\"e\":2,\"m\":3}\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model = BiRNN(EMBEDDING_DIM, \n",
    "                      HIDDEN_DIM, \n",
    "                      len(vocab_with_id), \n",
    "                      len(label_with_id),\n",
    "                      pre_trained_embedding).float()\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.3)\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        print(epoch)\n",
    "        train_num = len(train_X)\n",
    "        batch_size = BATCH_SIZE\n",
    "\n",
    "        #shuffle\n",
    "        combined=list(zip(train_X,train_y))\n",
    "        random.shuffle(combined)\n",
    "        train_X,train_y=zip(*combined)\n",
    "\n",
    "        if train_num % batch_size == 0:\n",
    "            total_batch = train_num // batch_size \n",
    "        else:\n",
    "            total_batch = train_num // batch_size + 1\n",
    "        \n",
    "        loss_total = 0\n",
    "        gold_list = np.array([-1])\n",
    "        pred_list = np.array([-1])\n",
    "        \n",
    "        for batch_id in range(total_batch):\n",
    "            #それぞれのsizeが違ってdata_loaderの使いかたがわからなかった\n",
    "            start = batch_id * batch_size\n",
    "            end = (batch_id + 1) * batch_size\n",
    "            sentences = train_X[start:end]\n",
    "            tags = train_y[start:end]\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            sentence_in = prepare_sequence(sentences, vocab_with_id,\"X\")\n",
    "            targets = prepare_sequence(tags, label_with_id,\"y\")\n",
    "            sentence_in, targets, word_seq_lengths = sequence2padded_tesnsor(sentence_in,targets,device)\n",
    "\n",
    "            tag_scores = model(sentence_in,word_seq_lengths)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            \n",
    "            # print(tag_scores)\n",
    "            loss_total += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                pred_y = torch.argmax(tag_scores,dim=1).cpu().numpy()\n",
    "            else:\n",
    "                pred_y = torch.argmax(tag_scores,dim=1).numpy()\n",
    "            pred_list = np.insert(pred_list,-1,pred_y)\n",
    "        \n",
    "            gold_y = np.array(targets.cpu())\n",
    "            gold_list = np.insert(gold_list,-1,gold_y)\n",
    "            \n",
    "        pred_list = np.delete(pred_list,-1)\n",
    "        gold_list = np.delete(gold_list,-1)\n",
    "        \n",
    "        train_acc = accuracy_score(gold_list, pred_list)\n",
    "\n",
    "        print(\"train_acc :\",train_acc)        \n",
    "        print(\"train_loss :\",loss_total)\n",
    "\n",
    "        #========================================================\n",
    "        #dev\n",
    "        dev_X, dev_y = data_import(\"dev\",vocab_with_id)\n",
    "        train_num = len(dev_X)\n",
    "        batch_size = BATCH_SIZE\n",
    "\n",
    "        if train_num % batch_size == 0:\n",
    "            total_batch = train_num // batch_size \n",
    "        else:\n",
    "            total_batch = train_num // batch_size + 1\n",
    "\n",
    "        dev_loss_total = 0\n",
    "        gold_list = np.array([-1])\n",
    "        pred_list = np.array([-1])\n",
    "\n",
    "        model.eval()\n",
    "        for batch_id in range(total_batch):\n",
    "            #それぞれのsizeが違ってdata_loaderの使いかたがわからなかった\n",
    "            start = batch_id * batch_size\n",
    "            end = (batch_id + 1) * batch_size\n",
    "            sentences = dev_X[start:end]\n",
    "            tags = dev_y[start:end]\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            sentence_in = prepare_sequence(sentences, vocab_with_id,\"X\")\n",
    "            targets = prepare_sequence(tags, label_with_id,\"y\")\n",
    "            sentence_in, targets, word_seq_lengths = sequence2padded_tesnsor(sentence_in,targets,device)\n",
    "\n",
    "            tag_scores = model(sentence_in,word_seq_lengths)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            \n",
    "            # print(tag_scores)\n",
    "            dev_loss_total += loss.item()\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                pred_y = torch.argmax(tag_scores,dim=1).cpu().numpy()\n",
    "            else:\n",
    "                pred_y = torch.argmax(tag_scores,dim=1).numpy()\n",
    "            pred_list = np.insert(pred_list,-1,pred_y)\n",
    "        \n",
    "            gold_y = np.array(targets.cpu())\n",
    "            gold_list = np.insert(gold_list,-1,gold_y)\n",
    "            \n",
    "        pred_list = np.delete(pred_list,-1)\n",
    "        gold_list = np.delete(gold_list,-1)\n",
    "        \n",
    "        dev_acc = accuracy_score(gold_list, pred_list)\n",
    "\n",
    "        print(\"dev_acc :\",dev_acc)        \n",
    "        print(\"devloss :\",dev_loss_total)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 86. 畳み込みニューラルネットワーク (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID番号で表現された単語列x=(x1,x2,…,xT)\n",
    "がある．ただし，T\n",
    "は単語列の長さ，xt∈ℝV\n",
    "は単語のID番号のone-hot表記である（V\n",
    "は単語の総数である）．畳み込みニューラルネットワーク（CNN: Convolutional Neural Network）を用い，単語列x\n",
    "からカテゴリy\n",
    "を予測するモデルを実装せよ．\n",
    "\n",
    "ただし，畳み込みニューラルネットワークの構成は以下の通りとする．\n",
    "\n",
    "単語埋め込みの次元数: dw\n",
    "畳み込みのフィルターのサイズ: 3 トークン\n",
    "畳み込みのストライド: 1 トークン\n",
    "畳み込みのパディング: あり\n",
    "畳み込み演算後の各時刻のベクトルの次元数: dh\n",
    "畳み込み演算後に最大値プーリング（max pooling）を適用し，入力文をdh\n",
    "次元の隠れベクトルで表現\n",
    "すなわち，時刻t\n",
    "の特徴ベクトルpt∈ℝdh\n",
    "は次式で表される．\n",
    "\n",
    "pt=g(W(px)[emb(xt−1);emb(xt);emb(xt+1)]+b(p))\n",
    "ただし，W(px)∈ℝdh×3dw,b(p)∈ℝdh\n",
    "はCNNのパラメータ，g\n",
    "は活性化関数（例えばtanh\n",
    "やReLUなど），[a;b;c]\n",
    "はベクトルa,b,c\n",
    "の連結である．なお，行列W(px)\n",
    "の列数が3dw\n",
    "になるのは，3個のトークンの単語埋め込みを連結したものに対して，線形変換を行うためである．\n",
    "\n",
    "最大値プーリングでは，特徴ベクトルの次元毎に全時刻における最大値を取り，入力文書の特徴ベクトルc∈ℝdh\n",
    "を求める．c[i]\n",
    "でベクトルc\n",
    "のi\n",
    "番目の次元の値を表すことにすると，最大値プーリングは次式で表される．\n",
    "\n",
    "c[i]=max1≤t≤Tpt[i]\n",
    "最後に，入力文書の特徴ベクトルc\n",
    "に行列W(yc)∈ℝL×dh\n",
    "とバイアス項b(y)∈ℝL\n",
    "による線形変換とソフトマックス関数を適用し，カテゴリy\n",
    "を予測する．\n",
    "\n",
    "y=softmax(W(yc)c+b(y))\n",
    "なお，この問題ではモデルの学習を行わず，ランダムに初期化された重み行列でy\n",
    "を計算するだけでよい．\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 87. 確率的勾配降下法によるCNNの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題86で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "train_acc : 0.8213749060856499\n",
      "train_loss : 160.4285215958953\n",
      "dev_acc : 0.875\n",
      "devloss : 15.079336039721966\n",
      "1\n",
      "train_acc : 0.912847483095417\n",
      "train_loss : 83.39828199520707\n",
      "dev_acc : 0.8802395209580839\n",
      "devloss : 14.821674607694149\n",
      "2\n",
      "train_acc : 0.9399887302779865\n",
      "train_loss : 55.47876461967826\n",
      "dev_acc : 0.8847305389221557\n",
      "devloss : 14.323731660842896\n",
      "3\n",
      "train_acc : 0.9666604057099925\n",
      "train_loss : 34.23760186415166\n",
      "dev_acc : 0.8907185628742516\n",
      "devloss : 14.618648897856474\n",
      "4\n",
      "train_acc : 0.9813110443275732\n",
      "train_loss : 21.260149023262784\n",
      "dev_acc : 0.8952095808383234\n",
      "devloss : 14.761813420802355\n",
      "5\n",
      "train_acc : 0.9894815927873779\n",
      "train_loss : 12.155366194667295\n",
      "dev_acc : 0.8974550898203593\n",
      "devloss : 15.761664882302284\n",
      "6\n",
      "train_acc : 0.9954921111945906\n",
      "train_loss : 7.967729117721319\n",
      "dev_acc : 0.8989520958083832\n",
      "devloss : 17.14870515652001\n",
      "7\n",
      "train_acc : 0.9969947407963937\n",
      "train_loss : 5.800838275579736\n",
      "dev_acc : 0.8959580838323353\n",
      "devloss : 17.413154434412718\n",
      "8\n",
      "train_acc : 0.9970886551465064\n",
      "train_loss : 5.037736271682661\n",
      "dev_acc : 0.8929640718562875\n",
      "devloss : 19.920050007291138\n",
      "9\n",
      "train_acc : 0.9969947407963937\n",
      "train_loss : 5.384438988781767\n",
      "dev_acc : 0.8937125748502994\n",
      "devloss : 19.17087087035179\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import os \n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence,pad_sequence,pack_sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "import gensim\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size,pre_trained_embedding):\n",
    "        super(CNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.word_embeddings.weight = nn.Parameter(torch.from_numpy(pre_trained_embedding))\n",
    "        self.cnn = nn.Conv1d(embedding_dim,hidden_dim,3,stride=1, padding=2)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim,tagset_size)\n",
    "\n",
    "    def forward(self,sentence,word_seq_lengths):\n",
    "        #sentence (batch_size,max_length) \n",
    "        embeds = self.word_embeddings(sentence)  \n",
    "        #embeds (batch_size,max_length,emb_dim)\n",
    "        embeds = embeds.permute(0, 2, 1)\n",
    "        #embeds (batch_size,emb_dim,max_length)\n",
    "        cnn_out =  F.relu(self.cnn(embeds))\n",
    "        #cnn_out (batch_size,hidden_dim,max_length-3+1)\n",
    "        pooled = F.max_pool1d(cnn_out,cnn_out.shape[2]).squeeze(2)\n",
    "        #pooled (batch.size,hidden_dim)\n",
    "        tag_space  = self.hidden2tag(pooled)\n",
    "        tag_scores = F.log_softmax(tag_space,dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "#vocab 次元の辞書を作る 80番の内容\n",
    "def make_vocab(data_path):\n",
    "    vocab = {}\n",
    "    with open(data_path) as data:\n",
    "        for line in data:\n",
    "            text_data = line.split(\"\\t\")[0]\n",
    "            words = text_data.split()\n",
    "\n",
    "            for word in  words:\n",
    "                if word in vocab:\n",
    "                    vocab[word] +=1\n",
    "                else:\n",
    "                    vocab[word] = 1\n",
    "\n",
    "    #(単語,頻度)の順にタプルで入ってる\n",
    "    vocab_sorted = sorted(vocab.items(),key=lambda x:x[1])[::-1]\n",
    "\n",
    "    vocab_with_id = {}\n",
    "    #idに変換\n",
    "    for item in vocab_sorted:\n",
    "        #default_dictを使って一行で書く\n",
    "        if item[1] >= 2:\n",
    "            vocab_with_id[item[0]] = len(vocab_with_id.items())+2\n",
    "        else:\n",
    "            vocab_with_id[item[0]] = 0\n",
    "\n",
    "    return vocab_with_id\n",
    "\n",
    "#train,test,devを読み込む関数\n",
    "def data_import(mode,vocab_with_id):\n",
    "    cu_path = os.getcwd()\n",
    "    name = mode\n",
    "    data_path = os.path.join(cu_path,\"data\",name+\".txt\")\n",
    "\n",
    "    text_data_list = []\n",
    "    label_list = []\n",
    "    with open(data_path) as data:\n",
    "        for line in data:\n",
    "            text_data,label  = line.strip().split(\"\\t\")\n",
    "            words = text_data.split()\n",
    "            text_data_list.append(words)\n",
    "            label_list.append(label)\n",
    "    \n",
    "    return text_data_list,label_list\n",
    "\n",
    "#単語列 -> id に変換する\n",
    "#ここでID errorの時に-1に処理するプログラムに変える\n",
    "def prepare_sequence(seqs,with_id,mode):\n",
    "    bacth_id_list = []\n",
    "    for seq in seqs:\n",
    "        idxs = [with_id[w] if (w in with_id) else 0 for w in seq]\n",
    "        bacth_id_list.append(idxs)\n",
    "    # print(bacth_id_list)\n",
    "    # return torch.tensor(idxs, dtype=torch.long)\n",
    "    return bacth_id_list\n",
    "\n",
    "#torch.saveはnumpyでもできる\n",
    "#data => torch.saveでsave それをtorch.loadで読み込むと早くなるかも\n",
    "\n",
    "#データセットをある程度長さごとに固めてバッチを作ると系列長が揃って処理時間が短くなる\n",
    "#Allen_NLPを使うと便利\n",
    "\n",
    "#結局手で書いてしまった\n",
    "#系列長が違う生でlist > tensor の変換がうまくいかないので0埋めを手動でやった\n",
    "#paddingには1番を割り当てる\n",
    "#unkは0番を割り当てる\n",
    "def sequence2padded_tesnsor(seqs,labels,device):    \n",
    "    batch_size = len(seqs)\n",
    "    word_seq_lengths = torch.LongTensor(list(map(len, seqs)))\n",
    "    max_seq_len = word_seq_lengths.max().item()\n",
    "    word_seq_tensor = torch.ones((batch_size, max_seq_len), requires_grad=True).long()\n",
    "    label_seq_tensor = torch.zeros((batch_size), requires_grad=True).long()\n",
    "    # mask = torch.zeros((batch_size, max_seq_len), requires_grad=True).byte()\n",
    "    for idx, (seq, label,seqlen) in enumerate(zip(seqs, labels,word_seq_lengths)):\n",
    "        seqlen = seqlen.item()\n",
    "        word_seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "        label_seq_tensor[idx] = torch.LongTensor(label)\n",
    "        # mask[idx, :seqlen] = torch.Tensor([1] * seqlen)\n",
    "    \n",
    "    word_seq_lengths, word_perm_idx = word_seq_lengths.sort(0, descending=True)\n",
    "    word_seq_tensor = word_seq_tensor[word_perm_idx]#長さごとにに並べ替え\n",
    "    label_seq_tensor = label_seq_tensor[word_perm_idx]\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        word_seq_tensor = word_seq_tensor.to(device)\n",
    "        label_seq_tensor = label_seq_tensor.to(device)\n",
    "        word_seq_lengths = word_seq_lengths.to(device)\n",
    "\n",
    "    return word_seq_tensor,label_seq_tensor,word_seq_lengths\n",
    "\n",
    "def init_embedding(vocab_with_id,EMBEDDING_DIM):\n",
    "    from gensim.models import KeyedVectors\n",
    "    #作ったvocabのindexに紐付ける\n",
    "    matrix = np.random.uniform(0, 1, size=(len(vocab_with_id)+1, EMBEDDING_DIM))\n",
    "    path = 'data/GoogleNews-vectors-negative300.bin'\n",
    "    vectors = KeyedVectors.load_word2vec_format(path,binary=True)\n",
    "    for word,id in vocab_with_id.items():\n",
    "        if word in vectors.vocab:\n",
    "            matrix[id] = vectors[word]\n",
    "    return matrix\n",
    "        \n",
    "def main():\n",
    "    #paraameter\n",
    "    EMBEDDING_DIM = 300\n",
    "    HIDDEN_DIM = 50\n",
    "    BATCH_SIZE = 32\n",
    "     \n",
    "    cu_path = os.getcwd()\n",
    "    data_path = os.path.join(cu_path,'data','train.txt')\n",
    "    vocab_with_id = make_vocab(data_path)\n",
    "    pre_trained_embedding = init_embedding(vocab_with_id,EMBEDDING_DIM)\n",
    "    \n",
    "    train_X, train_y = data_import(\"train\",vocab_with_id)\n",
    "\n",
    "    label_with_id = {\"b\":0,\"t\":1,\"e\":2,\"m\":3}\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \"\"\"\n",
    "    model = BiRNN(EMBEDDING_DIM, \n",
    "                      HIDDEN_DIM, \n",
    "                      len(vocab_with_id), \n",
    "                      len(label_with_id),\n",
    "                      pre_trained_embedding).float()\n",
    "    \"\"\"\n",
    "    model = CNN(EMBEDDING_DIM, \n",
    "                    HIDDEN_DIM, \n",
    "                    len(vocab_with_id), \n",
    "                    len(label_with_id),\n",
    "                    pre_trained_embedding).float()\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.3)\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        print(epoch)\n",
    "        train_num = len(train_X)\n",
    "        batch_size = BATCH_SIZE\n",
    "\n",
    "        #shuffle\n",
    "        combined=list(zip(train_X,train_y))\n",
    "        random.shuffle(combined)\n",
    "        train_X,train_y=zip(*combined)\n",
    "\n",
    "        if train_num % batch_size == 0:\n",
    "            total_batch = train_num // batch_size \n",
    "        else:\n",
    "            total_batch = train_num // batch_size + 1\n",
    "        \n",
    "        loss_total = 0\n",
    "        gold_list = np.array([-1])\n",
    "        pred_list = np.array([-1])\n",
    "        \n",
    "        for batch_id in range(total_batch):\n",
    "            #それぞれのsizeが違ってdata_loaderの使いかたがわからなかった\n",
    "            start = batch_id * batch_size\n",
    "            end = (batch_id + 1) * batch_size\n",
    "            sentences = train_X[start:end]\n",
    "            tags = train_y[start:end]\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            sentence_in = prepare_sequence(sentences, vocab_with_id,\"X\")\n",
    "            targets = prepare_sequence(tags, label_with_id,\"y\")\n",
    "            sentence_in, targets, word_seq_lengths = sequence2padded_tesnsor(sentence_in,targets,device)\n",
    "\n",
    "            tag_scores = model(sentence_in,word_seq_lengths)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            \n",
    "            # print(tag_scores)\n",
    "            loss_total += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                pred_y = torch.argmax(tag_scores,dim=1).cpu().numpy()\n",
    "            else:\n",
    "                pred_y = torch.argmax(tag_scores,dim=1).numpy()\n",
    "            pred_list = np.insert(pred_list,-1,pred_y)\n",
    "        \n",
    "            gold_y = np.array(targets.cpu())\n",
    "            gold_list = np.insert(gold_list,-1,gold_y)\n",
    "            \n",
    "        pred_list = np.delete(pred_list,-1)\n",
    "        gold_list = np.delete(gold_list,-1)\n",
    "        \n",
    "        train_acc = accuracy_score(gold_list, pred_list)\n",
    "\n",
    "        print(\"train_acc :\",train_acc)        \n",
    "        print(\"train_loss :\",loss_total)\n",
    "\n",
    "        #========================================================\n",
    "        #dev\n",
    "        dev_X, dev_y = data_import(\"dev\",vocab_with_id)\n",
    "        train_num = len(dev_X)\n",
    "        batch_size = BATCH_SIZE\n",
    "\n",
    "        if train_num % batch_size == 0:\n",
    "            total_batch = train_num // batch_size \n",
    "        else:\n",
    "            total_batch = train_num // batch_size + 1\n",
    "\n",
    "        dev_loss_total = 0\n",
    "        gold_list = np.array([-1])\n",
    "        pred_list = np.array([-1])\n",
    "\n",
    "        model.eval()\n",
    "        for batch_id in range(total_batch):\n",
    "            #それぞれのsizeが違ってdata_loaderの使いかたがわからなかった\n",
    "            start = batch_id * batch_size\n",
    "            end = (batch_id + 1) * batch_size\n",
    "            sentences = dev_X[start:end]\n",
    "            tags = dev_y[start:end]\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            sentence_in = prepare_sequence(sentences, vocab_with_id,\"X\")\n",
    "            targets = prepare_sequence(tags, label_with_id,\"y\")\n",
    "            sentence_in, targets, word_seq_lengths = sequence2padded_tesnsor(sentence_in,targets,device)\n",
    "\n",
    "            tag_scores = model(sentence_in,word_seq_lengths)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            \n",
    "            # print(tag_scores)\n",
    "            dev_loss_total += loss.item()\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                pred_y = torch.argmax(tag_scores,dim=1).cpu().numpy()\n",
    "            else:\n",
    "                pred_y = torch.argmax(tag_scores,dim=1).numpy()\n",
    "            pred_list = np.insert(pred_list,-1,pred_y)\n",
    "        \n",
    "            gold_y = np.array(targets.cpu())\n",
    "            gold_list = np.insert(gold_list,-1,gold_y)\n",
    "            \n",
    "        pred_list = np.delete(pred_list,-1)\n",
    "        gold_list = np.delete(gold_list,-1)\n",
    "        \n",
    "        dev_acc = accuracy_score(gold_list, pred_list)\n",
    "\n",
    "        print(\"dev_acc :\",dev_acc)        \n",
    "        print(\"devloss :\",dev_loss_total)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  88. パラメータチューニングPerma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "train_acc : 0.7958302028549963\n",
      "train_loss : 184.97623272240162\n",
      "dev_acc : 0.8622754491017964\n",
      "devloss : 17.325629472732544\n",
      "1\n",
      "train_acc : 0.8942524417731029\n",
      "train_loss : 96.6865854114294\n",
      "dev_acc : 0.8869760479041916\n",
      "devloss : 13.346505269408226\n",
      "2\n",
      "train_acc : 0.9293764087152517\n",
      "train_loss : 66.28478169068694\n",
      "dev_acc : 0.8824850299401198\n",
      "devloss : 14.125717639923096\n",
      "3\n",
      "train_acc : 0.9517280240420737\n",
      "train_loss : 46.61705727502704\n",
      "dev_acc : 0.8937125748502994\n",
      "devloss : 12.806960821151733\n",
      "4\n",
      "train_acc : 0.9691021788129226\n",
      "train_loss : 32.09472524607554\n",
      "dev_acc : 0.8847305389221557\n",
      "devloss : 13.672567173838615\n",
      "5\n",
      "train_acc : 0.9822501878287002\n",
      "train_loss : 20.12522188294679\n",
      "dev_acc : 0.8989520958083832\n",
      "devloss : 13.569524727761745\n",
      "6\n",
      "train_acc : 0.9896694214876033\n",
      "train_loss : 14.226403721026145\n",
      "dev_acc : 0.8967065868263473\n",
      "devloss : 14.01552214473486\n",
      "7\n",
      "train_acc : 0.9932381667918858\n",
      "train_loss : 10.39991925819777\n",
      "dev_acc : 0.8944610778443114\n",
      "devloss : 14.778943970799446\n",
      "8\n",
      "train_acc : 0.9953042824943651\n",
      "train_loss : 7.497202829224989\n",
      "dev_acc : 0.8907185628742516\n",
      "devloss : 15.068840220570564\n",
      "9\n",
      "train_acc : 0.9966190833959429\n",
      "train_loss : 6.454101771058049\n",
      "dev_acc : 0.9034431137724551\n",
      "devloss : 15.353960387408733\n",
      "10\n",
      "train_acc : 0.996900826446281\n",
      "train_loss : 5.64889807876898\n",
      "dev_acc : 0.8989520958083832\n",
      "devloss : 14.828989386558533\n",
      "11\n",
      "train_acc : 0.9982156273478587\n",
      "train_loss : 4.010805131372763\n",
      "dev_acc : 0.8952095808383234\n",
      "devloss : 15.848756838589907\n",
      "12\n",
      "train_acc : 0.9982156273478587\n",
      "train_loss : 4.384977206675103\n",
      "dev_acc : 0.9019461077844312\n",
      "devloss : 15.592569351196289\n",
      "13\n",
      "train_acc : 0.9981217129977461\n",
      "train_loss : 3.495413805823773\n",
      "dev_acc : 0.8944610778443114\n",
      "devloss : 16.16572367399931\n",
      "14\n",
      "train_acc : 0.9983095416979715\n",
      "train_loss : 3.6939041168952826\n",
      "dev_acc : 0.8974550898203593\n",
      "devloss : 15.887086249887943\n",
      "final test score is  0.9190962099125365\n"
     ]
    }
   ],
   "source": [
    "#88\n",
    "#CNNの方がベースの性能が高そうだったのでCNNを改造する\n",
    "#3~5のカーネルサイズの情報を全部使う\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size,filter_sizes,dropout_rate,pre_trained_embedding):\n",
    "        super(CNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.word_embeddings.weight = nn.Parameter(torch.from_numpy(pre_trained_embedding))\n",
    "        self.cnn_list = nn.ModuleList([\n",
    "                                    nn.Conv1d(in_channels = embedding_dim, \n",
    "                                              out_channels = hidden_dim, \n",
    "                                              kernel_size = fs,\n",
    "                                              stride=1,\n",
    "                                              padding = 2)\n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        self.hidden2tag = nn.Linear(len(filter_sizes) * hidden_dim,tagset_size)\n",
    "        self.dropout =  nn.Dropout(dropout_rate) \n",
    "\n",
    "    def forward(self,sentence,word_seq_lengths):\n",
    "        #sentence (batch_size,max_length) \n",
    "        embeds = self.word_embeddings(sentence)  \n",
    "        #embeds (batch_size,max_length,emb_dim)\n",
    "        embeds = embeds.permute(0, 2, 1)\n",
    "        #embeds (batch_size,emb_dim,max_length)\n",
    "        cnn_out_list =  [F.relu(cnn(embeds)) for cnn in self.cnn_list] \n",
    "        #cnn_out [(batch_size,hidden_dim,max_length+)]*len(filter_sizes)\n",
    "        pooled = [F.max_pool1d(cnn_out,cnn_out.shape[2]).squeeze(2) for cnn_out in cnn_out_list]\n",
    "        #pooled [(batch.size,hidden_dim)]*len(filter_sizes)\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        #cat = (batch_size, hiddem_size * len(filter_sizes))\n",
    "        tag_space  = self.hidden2tag(cat)\n",
    "        tag_scores = F.log_softmax(tag_space,dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "#vocab 次元の辞書を作る 80番の内容\n",
    "def make_vocab(data_path):\n",
    "    vocab = {}\n",
    "    with open(data_path) as data:\n",
    "        for line in data:\n",
    "            text_data = line.split(\"\\t\")[0]\n",
    "            words = text_data.split()\n",
    "\n",
    "            for word in  words:\n",
    "                if word in vocab:\n",
    "                    vocab[word] +=1\n",
    "                else:\n",
    "                    vocab[word] = 1\n",
    "\n",
    "    #(単語,頻度)の順にタプルで入ってる\n",
    "    vocab_sorted = sorted(vocab.items(),key=lambda x:x[1])[::-1]\n",
    "\n",
    "    vocab_with_id = {}\n",
    "    #idに変換\n",
    "    for item in vocab_sorted:\n",
    "        #default_dictを使って一行で書く\n",
    "        if item[1] >= 2:\n",
    "            vocab_with_id[item[0]] = len(vocab_with_id.items())+2\n",
    "        else:\n",
    "            vocab_with_id[item[0]] = 0\n",
    "\n",
    "    return vocab_with_id\n",
    "\n",
    "#train,test,devを読み込む関数\n",
    "def data_import(mode,vocab_with_id):\n",
    "    cu_path = os.getcwd()\n",
    "    name = mode\n",
    "    data_path = os.path.join(cu_path,\"data\",name+\".txt\")\n",
    "\n",
    "    text_data_list = []\n",
    "    label_list = []\n",
    "    with open(data_path) as data:\n",
    "        for line in data:\n",
    "            text_data,label  = line.strip().split(\"\\t\")\n",
    "            words = text_data.split()\n",
    "            text_data_list.append(words)\n",
    "            label_list.append(label)\n",
    "    \n",
    "    return text_data_list,label_list\n",
    "\n",
    "#単語列 -> id に変換する\n",
    "#ここでID errorの時に-1に処理するプログラムに変える\n",
    "def prepare_sequence(seqs,with_id,mode):\n",
    "    bacth_id_list = []\n",
    "    for seq in seqs:\n",
    "        idxs = [with_id[w] if (w in with_id) else 0 for w in seq]\n",
    "        bacth_id_list.append(idxs)\n",
    "    # print(bacth_id_list)\n",
    "    # return torch.tensor(idxs, dtype=torch.long)\n",
    "    return bacth_id_list\n",
    "\n",
    "#torch.saveはnumpyでもできる\n",
    "#data => torch.saveでsave それをtorch.loadで読み込むと早くなるかも\n",
    "\n",
    "#データセットをある程度長さごとに固めてバッチを作ると系列長が揃って処理時間が短くなる\n",
    "#Allen_NLPを使うと便利\n",
    "\n",
    "#結局手で書いてしまった\n",
    "#系列長が違う生でlist > tensor の変換がうまくいかないので0埋めを手動でやった\n",
    "#paddingには1番を割り当てる\n",
    "#unkは0番を割り当てる\n",
    "def sequence2padded_tesnsor(seqs,labels,device):    \n",
    "    batch_size = len(seqs)\n",
    "    word_seq_lengths = torch.LongTensor(list(map(len, seqs)))\n",
    "    max_seq_len = word_seq_lengths.max().item()\n",
    "    word_seq_tensor = torch.ones((batch_size, max_seq_len), requires_grad=True).long()\n",
    "    label_seq_tensor = torch.zeros((batch_size), requires_grad=True).long()\n",
    "    # mask = torch.zeros((batch_size, max_seq_len), requires_grad=True).byte()\n",
    "    for idx, (seq, label,seqlen) in enumerate(zip(seqs, labels,word_seq_lengths)):\n",
    "        seqlen = seqlen.item()\n",
    "        word_seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "        label_seq_tensor[idx] = torch.LongTensor(label)\n",
    "        # mask[idx, :seqlen] = torch.Tensor([1] * seqlen)\n",
    "    \n",
    "    word_seq_lengths, word_perm_idx = word_seq_lengths.sort(0, descending=True)\n",
    "    word_seq_tensor = word_seq_tensor[word_perm_idx]#長さごとにに並べ替え\n",
    "    label_seq_tensor = label_seq_tensor[word_perm_idx]\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        word_seq_tensor = word_seq_tensor.to(device)\n",
    "        label_seq_tensor = label_seq_tensor.to(device)\n",
    "        word_seq_lengths = word_seq_lengths.to(device)\n",
    "\n",
    "    return word_seq_tensor,label_seq_tensor,word_seq_lengths\n",
    "\n",
    "def init_embedding(vocab_with_id,EMBEDDING_DIM):\n",
    "    from gensim.models import KeyedVectors\n",
    "    #作ったvocabのindexに紐付ける\n",
    "    matrix = np.random.uniform(0, 1, size=(len(vocab_with_id)+1, EMBEDDING_DIM))\n",
    "    path = 'data/GoogleNews-vectors-negative300.bin'\n",
    "    vectors = KeyedVectors.load_word2vec_format(path,binary=True)\n",
    "    for word,id in vocab_with_id.items():\n",
    "        if word in vectors.vocab:\n",
    "            matrix[id] = vectors[word]\n",
    "    return matrix\n",
    "        \n",
    "def main():\n",
    "    #paraameter\n",
    "    EMBEDDING_DIM = 300\n",
    "    HIDDEN_DIM = 50\n",
    "    BATCH_SIZE = 32\n",
    "    filter_sizes = [3,4,5]\n",
    "    dropout_rate = 0.1\n",
    "    epoch_num = 15\n",
    "\n",
    "    cu_path = os.getcwd()\n",
    "    data_path = os.path.join(cu_path,'data','train.txt')\n",
    "    vocab_with_id = make_vocab(data_path)\n",
    "    pre_trained_embedding = init_embedding(vocab_with_id,EMBEDDING_DIM)\n",
    "    \n",
    "    train_X, train_y = data_import(\"train\",vocab_with_id)\n",
    "\n",
    "    label_with_id = {\"b\":0,\"t\":1,\"e\":2,\"m\":3}\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \"\"\"\n",
    "    model = BiRNN(EMBEDDING_DIM, \n",
    "                      HIDDEN_DIM, \n",
    "                      len(vocab_with_id), \n",
    "                      len(label_with_id),\n",
    "                      pre_trained_embedding).float()\n",
    "    \"\"\"\n",
    "    model = CNN(EMBEDDING_DIM, \n",
    "                    HIDDEN_DIM, \n",
    "                    len(vocab_with_id), \n",
    "                    len(label_with_id),\n",
    "                    filter_sizes,\n",
    "                    dropout_rate,\n",
    "                    pre_trained_embedding).float()\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.2)\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    prev_acc = -1\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        model.train()\n",
    "        print(epoch)\n",
    "        train_num = len(train_X)\n",
    "        batch_size = BATCH_SIZE\n",
    "\n",
    "        #shuffle\n",
    "        combined=list(zip(train_X,train_y))\n",
    "        random.shuffle(combined)\n",
    "        train_X,train_y=zip(*combined)\n",
    "\n",
    "        if train_num % batch_size == 0:\n",
    "            total_batch = train_num // batch_size \n",
    "        else:\n",
    "            total_batch = train_num // batch_size + 1\n",
    "        \n",
    "        loss_total = 0\n",
    "        gold_list = np.array([-1])\n",
    "        pred_list = np.array([-1])\n",
    "        \n",
    "        for batch_id in range(total_batch):\n",
    "            #それぞれのsizeが違ってdata_loaderの使いかたがわからなかった\n",
    "            start = batch_id * batch_size\n",
    "            end = (batch_id + 1) * batch_size\n",
    "            sentences = train_X[start:end]\n",
    "            tags = train_y[start:end]\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            sentence_in = prepare_sequence(sentences, vocab_with_id,\"X\")\n",
    "            targets = prepare_sequence(tags, label_with_id,\"y\")\n",
    "            sentence_in, targets, word_seq_lengths = sequence2padded_tesnsor(sentence_in,targets,device)\n",
    "\n",
    "            tag_scores = model(sentence_in,word_seq_lengths)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            \n",
    "            # print(tag_scores)\n",
    "            loss_total += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                pred_y = torch.argmax(tag_scores,dim=1).cpu().numpy()\n",
    "            else:\n",
    "                pred_y = torch.argmax(tag_scores,dim=1).numpy()\n",
    "            pred_list = np.insert(pred_list,-1,pred_y)\n",
    "        \n",
    "            gold_y = np.array(targets.cpu())\n",
    "            gold_list = np.insert(gold_list,-1,gold_y)\n",
    "            \n",
    "        pred_list = np.delete(pred_list,-1)\n",
    "        gold_list = np.delete(gold_list,-1)\n",
    "        \n",
    "        train_acc = accuracy_score(gold_list, pred_list)\n",
    "\n",
    "        print(\"train_acc :\",train_acc)        \n",
    "        print(\"train_loss :\",loss_total)\n",
    "\n",
    "        #========================================================\n",
    "        #dev\n",
    "        dev_X, dev_y = data_import(\"dev\",vocab_with_id)\n",
    "        train_num = len(dev_X)\n",
    "        batch_size = BATCH_SIZE\n",
    "\n",
    "        if train_num % batch_size == 0:\n",
    "            total_batch = train_num // batch_size \n",
    "        else:\n",
    "            total_batch = train_num // batch_size + 1\n",
    "\n",
    "        dev_loss_total = 0\n",
    "        gold_list = np.array([-1])\n",
    "        pred_list = np.array([-1])\n",
    "\n",
    "        model.eval()\n",
    "        for batch_id in range(total_batch):\n",
    "            #それぞれのsizeが違ってdata_loaderの使いかたがわからなかった\n",
    "            start = batch_id * batch_size\n",
    "            end = (batch_id + 1) * batch_size\n",
    "            sentences = dev_X[start:end]\n",
    "            tags = dev_y[start:end]\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            sentence_in = prepare_sequence(sentences, vocab_with_id,\"X\")\n",
    "            targets = prepare_sequence(tags, label_with_id,\"y\")\n",
    "            sentence_in, targets, word_seq_lengths = sequence2padded_tesnsor(sentence_in,targets,device)\n",
    "\n",
    "            tag_scores = model(sentence_in,word_seq_lengths)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            \n",
    "            # print(tag_scores)\n",
    "            dev_loss_total += loss.item()\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                pred_y = torch.argmax(tag_scores,dim=1).cpu().numpy()\n",
    "            else:\n",
    "                pred_y = torch.argmax(tag_scores,dim=1).numpy()\n",
    "            pred_list = np.insert(pred_list,-1,pred_y)\n",
    "        \n",
    "            gold_y = np.array(targets.cpu())\n",
    "            gold_list = np.insert(gold_list,-1,gold_y)\n",
    "            \n",
    "        pred_list = np.delete(pred_list,-1)\n",
    "        gold_list = np.delete(gold_list,-1)\n",
    "        \n",
    "        dev_acc = accuracy_score(gold_list, pred_list)\n",
    "\n",
    "        print(\"dev_acc :\",dev_acc)        \n",
    "        print(\"devloss :\",dev_loss_total)\n",
    "\n",
    "        if dev_acc > prev_acc:\n",
    "            prev_acc = dev_acc\n",
    "            model_name = \"work/model/\"+\"best.model\"\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "\n",
    "            optim_name = \"work/model/\"+str(epoch)+\".opt\"\n",
    "            torch.save(optimizer.state_dict(), optim_name)\n",
    "\n",
    "    #test\n",
    "    gold_list = np.array([-1])\n",
    "    pred_list = np.array([-1])\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model.load_state_dict(torch.load(model_name))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(model_name,map_location='cpu'))\n",
    "\n",
    "    test_X, test_y = data_import(\"test\",vocab_with_id)\n",
    "    sentence_in = prepare_sequence(test_X, vocab_with_id,\"X\")\n",
    "    targets = prepare_sequence(test_y, label_with_id,\"y\")\n",
    "    sentence_in, targets, word_seq_lengths = sequence2padded_tesnsor(sentence_in,targets,device)\n",
    "    tag_scores = model(sentence_in,word_seq_lengths)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        pred_y = torch.argmax(tag_scores,dim=1).cpu().numpy()\n",
    "    else:\n",
    "        pred_y = torch.argmax(tag_scores,dim=1).numpy()\n",
    "    pred_list = np.insert(pred_list,-1,pred_y)\n",
    "\n",
    "    gold_y = np.array(targets.cpu())\n",
    "    gold_list = np.insert(gold_list,-1,gold_y)\n",
    "\n",
    "    pred_list = np.delete(pred_list,-1)\n",
    "    gold_list = np.delete(gold_list,-1)\n",
    "\n",
    "    test_acc = accuracy_score(gold_list, pred_list)\n",
    "        \n",
    "    print(\"final test score is \",test_acc)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
